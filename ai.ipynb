{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit ('.venv': pipenv)"
  },
  "interpreter": {
   "hash": "b85dc5435e0bfe71fff45c7bd600e0d35505844357c06700d87c8785fb37e4d4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part A > Time Series Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Data Manipulation Dependencies\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Graphing Dependencies\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "# Time Series Dependency\r\n",
    "import statsmodels.tsa as time_series_analysis\r\n",
    "from pmdarima.arima import arima\r\n",
    "\r\n",
    "# \r\n",
    "from sklearn.cluster import KMeans\r\n",
    "\r\n",
    "from typing import Union, List, Tuple\r\n",
    "import warnings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "warnings.filterwarnings(action='ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from statsmodels.tsa.stattools import adfuller\r\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('./data/train.csv')\r\n",
    "df['Date'] = pd.to_datetime(df['Date'])\r\n",
    "df.set_index(keys='Date', inplace=True)\r\n",
    "df.sort_index(inplace=True)\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CO, NMHC, NOx, O3 = [df[df['Gas'] == gas] for gas in df['Gas'].unique()]\r\n",
    "\r\n",
    "CO.groupby(by=[pd.to_datetime(CO.index).year, pd.to_datetime(CO.index).quarter]).count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_data_grouped_by_date(df: pd.DataFrame, freq: Union[str, List[str]]):\r\n",
    "    def get_plot(df: pd.DataFrame, fr: str, ax):\r\n",
    "        df_tmp = df.copy()\r\n",
    "        gbo = df_tmp.resample(rule=fr).mean()\r\n",
    "\r\n",
    "        ax.plot(gbo.index, gbo['T'], label='T')\r\n",
    "        ax.plot(gbo.index, gbo['RH'], label='RH')\r\n",
    "        ax.plot([], [], color='g', label='Value')\r\n",
    "\r\n",
    "        ax2 = ax.twinx()\r\n",
    "        ax2.plot(gbo.index, gbo['Value'], color='green')\r\n",
    "        return ax\r\n",
    "    \r\n",
    "    if type(freq) is str or len(freq) == 1:\r\n",
    "        fig, ax = plt.subplots()\r\n",
    "        get_plot(df, freq if type(freq) is str else freq[0], ax) #type: ignore\r\n",
    "    else:\r\n",
    "        fig, ax = plt.subplots(nrows=len(freq), figsize=(8, 5))\r\n",
    "        for i, fr in enumerate(freq):\r\n",
    "            get_plot(df, fr, ax[i])\r\n",
    "        ax[-1].legend(bbox_to_anchor=(1.3, 3.3))\r\n",
    "\r\n",
    "get_data_grouped_by_date(df=NMHC, freq=['M', '2M', 'Q', 'Y'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_gas_distributions(df: pd.DataFrame):\r\n",
    "    counts = df.groupby(by='Gas').count().median(axis=1).astype(int)\r\n",
    "    sns.barplot(x=counts.index, y=counts, palette='deep')\r\n",
    "\r\n",
    "get_gas_distributions(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_(df):\r\n",
    "    # fig, ax = plt.subplots(nrows=2, ncols=2)\r\n",
    "    gb = df.groupby(by='Gas', as_index=False)\r\n",
    "    mean_ = gb.mean()\r\n",
    "    mean_['Type'] = ['Mean'] * mean_.shape[0]\r\n",
    "    types = ['Mean', 'Median']\r\n",
    "    comb_df = pd.DataFrame()\r\n",
    "    for i, frame in enumerate([gb.mean(), gb.median()]):\r\n",
    "        tmp_df = frame\r\n",
    "        tmp_df['Type'] = types[i]\r\n",
    "        comb_df = pd.concat(objs=(comb_df, tmp_df), axis=0)\r\n",
    "    # print(comb_df)\r\n",
    "    sns.barplot(data=comb_df, x='Gas', y='Value', hue='Type', palette='rainbow')\r\n",
    "plot_(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_stationarity_summary(dfs: Tuple[pd.DataFrame], plot: bool = False):\r\n",
    "    def get_stationarity_of_columns(df: pd.DataFrame, significance_level: float = 0.05):\r\n",
    "        df_tmp = df.copy()\r\n",
    "        stationarities = []\r\n",
    "        numerical_cols = []\r\n",
    "        for col in df.columns:\r\n",
    "            if df[col].dtype.kind in 'biufc':\r\n",
    "                numerical_cols.append(col)\r\n",
    "                p_value = round(adfuller(df_tmp.resample(rule='M').mean()[col])[1], 5)\r\n",
    "                if p_value < significance_level:\r\n",
    "                    stationarities.append(True)\r\n",
    "                    print(f'{col}:\\tStationary ({p_value})'.expandtabs(tabsize=10))\r\n",
    "                else:\r\n",
    "                    stationarities.append(False)\r\n",
    "                    print(f'{col}:\\tNon-Stationary ({p_value})'.expandtabs(tabsize=10))\r\n",
    "                if plot:\r\n",
    "                    seasonal_decompose(df_tmp.resample(rule='M')[col].mean()).plot()\r\n",
    "        return numerical_cols, stationarities\r\n",
    "\r\n",
    "    serieses = []\r\n",
    "    for gas in dfs:\r\n",
    "        print(gas['Gas'][0])\r\n",
    "        c, st = get_stationarity_of_columns(gas)\r\n",
    "        serieses.append(pd.Series(name=gas['Gas'][0], index=c, data=st))\r\n",
    "        print()\r\n",
    "    return pd.concat(objs=serieses, axis=1).T\r\n",
    "\r\n",
    "stats = get_stationarity_summary((CO, NMHC, NOx, O3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def groupby_date(df: pd.DataFrame, freq: str):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    gbo = df_tmp.resample(rule=freq).mean()\r\n",
    "    return gbo\r\n",
    "\r\n",
    "groupby_date(CO, 'M')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from statsmodels.tsa.stattools import pacf, acf\r\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\r\n",
    "\r\n",
    "def cf_summary(dfs: Tuple[pd.DataFrame], kind: Union['pacf', 'acf'], resample: str = 'M', threshold: float = 0.01, plot: bool = False):\r\n",
    "    corr_fn = pacf if kind == 'pacf' else acf\r\n",
    "    plot_fn = plot_pacf if kind == 'pacf' else plot_acf\r\n",
    "    gases = []\r\n",
    "    best_ts = []\r\n",
    "    for df in dfs:\r\n",
    "        current_gas = df['Gas'][0]\r\n",
    "        gases.append(current_gas)\r\n",
    "        if resample is not None:\r\n",
    "            gbo = groupby_date(df, resample)\r\n",
    "        else:\r\n",
    "            gbo = df.copy()\r\n",
    "        lag_space = int(gbo.count()[0] / 2) - 1\r\n",
    "        corr_scores, conf_intvs = corr_fn(gbo['Value'], nlags=lag_space, alpha=threshold)\r\n",
    "        lower_conf_bound = conf_intvs[:, 0] - corr_scores\r\n",
    "        upper_conf_bound = conf_intvs[:, 1] - corr_scores\r\n",
    "        t_values = np.where((corr_scores < lower_conf_bound) | (corr_scores > upper_conf_bound))[0][1:]\r\n",
    "        best_t = t_values[np.argmax((np.abs(corr_scores[t_values])))] if len(t_values) > 0 else 0\r\n",
    "        best_ts.append(best_t)\r\n",
    "        if plot:\r\n",
    "            plot_fn(gbo['Value'], lags=lag_space, alpha=threshold).get_axes()[0].set_title(f'{current_gas}: possible t-values: {tuple(t_values)} Best t: {best_t}')\r\n",
    "    return pd.DataFrame(data={\r\n",
    "        'Gas': gases,\r\n",
    "        't': best_ts\r\n",
    "    }).set_index(keys='Gas')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pacf_summary = cf_summary((CO, NMHC, NOx, O3), kind='pacf', threshold=0.01)\r\n",
    "print(pacf_summary)\r\n",
    "\r\n",
    "acf_summary = cf_summary((CO, NMHC, NOx, O3), kind='acf', threshold=0.01)\r\n",
    "print(acf_summary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from statsmodels.tsa.statespace.tools import diff\r\n",
    "\r\n",
    "def get_diff(df):\r\n",
    "    return groupby_date(df, 'M')['Value'], diff(groupby_date(df, 'M')['Value'])\r\n",
    "\r\n",
    "first, second = get_diff(NOx)\r\n",
    "\r\n",
    "plt.plot(first.index, first.values)\r\n",
    "plt.plot(second.index, second.values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trial1 = pd.DataFrame(data={'Value': second, 'Gas': ['NOx' for i in range(second.shape[0])]})\r\n",
    "trial2 = pd.DataFrame(data={'Value': first, 'Gas': ['NOx' for i in range(first.shape[0])]})\r\n",
    "\r\n",
    "cf_summary((trial1, trial2), kind='pacf', threshold=0.05, resample=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Loop thru gases, thresholds and freqs to get dataframe (sth like gridsearch but for time series)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def arima(df: pd.DataFrame, freq: str, order: Tuple[int, int, int], print_summary: bool = False):\r\n",
    "    return_df = groupby_date(df, freq)['Value']\r\n",
    "    model = time_series_analysis.arima_model.ARIMA(return_df, order=order)\r\n",
    "    res = model.fit()\r\n",
    "    if print_summary:\r\n",
    "        print(res.summary())\r\n",
    "    return return_df, res\r\n",
    "\r\n",
    "arima(df=NOx, freq='M', order=(1, 0, 1))[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_facility():\r\n",
    "    og, model = arima(df=NOx, freq='M', order=(2, 1, 0))\r\n",
    "    y_hat = model.predict(start=2, end=20)\r\n",
    "    tdf = pd.DataFrame(data={\r\n",
    "        'Value': y_hat,\r\n",
    "        'Color': np.isin(y_hat.index, NOx.index)\r\n",
    "    })\r\n",
    "    hv = tdf[tdf['Color']]\r\n",
    "    donthv = tdf[~tdf['Color']]\r\n",
    "    bridge = tdf[hv.index[-1]:donthv.index[0]]\r\n",
    "    ax = sns.lineplot(x=hv.index, y=hv['Value'], color='blue')\r\n",
    "    sns.lineplot(x=bridge.index, y=bridge['Value'], color='green', ax=ax)\r\n",
    "    sns.lineplot(x=donthv.index, y=donthv['Value'], color='green', ax=ax)\r\n",
    "    sns.lineplot(x=hv.index, y=og[hv.index], color='orange', ax=ax)\r\n",
    "\r\n",
    "test_facility()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pmdarima.arima import auto_arima\r\n",
    "\r\n",
    "def auto(df: pd.DataFrame, freq: str):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    # gbo = df_tmp.resample(rule=freq).mean()\r\n",
    "    arima_model = auto_arima(df_tmp['Value'], seasonal=False)\r\n",
    "    print(arima_model.summary())\r\n",
    "\r\n",
    "auto(O3, 'M')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# use supervised learning to check\r\n",
    "# use varima to check correlation between T and RH"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part B > Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Exclusive Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2 = pd.read_csv('./data/Mall_Customers.csv', index_col=0)\r\n",
    "df2.rename(mapper={'Genre': 'Gender'}, axis=1, inplace=True)\r\n",
    "df2.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "['Eigenvalue', 'Explained Variance', 'Cumulative Explained Variance'].extend(df2.drop(columns='Gender').columns.tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from typing import Union, List\r\n",
    "\r\n",
    "def get_pca_results(df: pd.DataFrame, ignore_cols: Union[str, List[str]]):\r\n",
    "    df_scaled = StandardScaler().fit_transform(X=df.drop(columns=ignore_cols))\r\n",
    "\r\n",
    "    pca = PCA(n_components=df_scaled.shape[1]).fit(X=df_scaled)\r\n",
    "    header = ['Eigenvalue', 'Explained Variance', 'Cumulative Explained Variance']\r\n",
    "    header.extend(df.drop(columns=ignore_cols).columns.tolist())\r\n",
    "    eigenvalues = pca.explained_variance_\r\n",
    "    eigenvectors = pca.components_\r\n",
    "    expl_var = pca.explained_variance_ratio_\r\n",
    "    cum_expl_var = pca.explained_variance_ratio_.cumsum()\r\n",
    "    pca_results = pd.DataFrame(\r\n",
    "        data=np.hstack((\r\n",
    "            eigenvalues.reshape(-1, 1),\r\n",
    "            expl_var.reshape(-1, 1),\r\n",
    "            cum_expl_var.reshape(-1, 1),\r\n",
    "            eigenvectors\r\n",
    "        )),\r\n",
    "        columns=header,\r\n",
    "        index=[f'PC {i + 1}' for i in range(df_scaled.shape[1])]\r\n",
    "    )\r\n",
    "\r\n",
    "    df_transformed = pd.DataFrame(\r\n",
    "        data=pca.transform(df_scaled),\r\n",
    "        index=df.index,\r\n",
    "        columns=[f'PC {i + 1}' for i in range(df_scaled.shape[1])]\r\n",
    "    )\r\n",
    "\r\n",
    "    return pca_results, df_transformed\r\n",
    "\r\n",
    "pca_results, df2_transformed = get_pca_results(df=df2, ignore_cols='Gender')\r\n",
    "pca_results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scree_plot(df: pd.DataFrame, pca: pd.DataFrame):\r\n",
    "    with sns.axes_style(style='darkgrid'):\r\n",
    "        ax = sns.pointplot(data=pca, x=pca.index, y=pca['Eigenvalue'])\r\n",
    "        ax.set(\r\n",
    "            title='Scree Plot for PCA (df2)',\r\n",
    "            ylim=(0, 1.4)\r\n",
    "        )\r\n",
    "        ax.annotate(text='As there is no elbow,\\nno PC should be discarded', xy=(1.75, 1.2), ha='center')\r\n",
    "        return ax\r\n",
    "\r\n",
    "scree_plot(df2, pca_results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_dist_score(df: pd.DataFrame, clusters: int):\r\n",
    "    model = KMeans(n_clusters=clusters).fit(X=df[['Annual Income (k$)', 'Spending Score (1-100)']])\r\n",
    "    centers = model.cluster_centers_\r\n",
    "    y_hat = model.predict(df[['Annual Income (k$)', 'Spending Score (1-100)']])\r\n",
    "    # plt.scatter(centers[:,0], centers[:,1])\r\n",
    "    # sns.scatterplot(x=df['Annual Income (k$)'], y=df['Spending Score (1-100)'], hue=y_hat)\r\n",
    "    df_tmp = pd.concat(objs=(df, pd.Series(data=y_hat, name='Class', index=df.index)), axis=1)\r\n",
    "    means = []\r\n",
    "    stds = []\r\n",
    "    count = []\r\n",
    "    for i, c in enumerate(centers):\r\n",
    "        # plt.annotate(text=str(model.predict(c.reshape(-1, 2))), xy=(c[0], c[1])) # type: ignore\r\n",
    "        class_i = df_tmp[df_tmp['Class'] == i][['Annual Income (k$)', 'Spending Score (1-100)']]\r\n",
    "        distances = np.linalg.norm(class_i - centers[i, :], axis=1)\r\n",
    "        # print(i, int(class_i.count(axis=0).mean()), distances.mean(), distances.std())\r\n",
    "        # count.append(int(class_i.count(axis=0).mean()))\r\n",
    "        means.append(distances.mean())\r\n",
    "        # stds.append(distances.std())\r\n",
    "    return np.std(means)\r\n",
    "\r\n",
    "get_dist_score(df2, 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# type: ignore\r\n",
    "from sklearn.metrics import silhouette_score\r\n",
    "from more_itertools import powerset\r\n",
    "\r\n",
    "def get_silhouette_score_plot(df: pd.DataFrame, ignore_cols: Union[str, List[str]] = None):\r\n",
    "    ignore_cols = ignore_cols if ignore_cols is not None else [] \r\n",
    "    col_combs = list(filter(lambda x: len(x) > 1, list(powerset(df.drop(columns=ignore_cols).columns))))\r\n",
    "    # n = len(cols)\r\n",
    "    # n = 10\r\n",
    "    # fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(10, 8))\r\n",
    "    # c = 0\r\n",
    "    fig, ax = plt.subplots(nrows=len(col_combs), figsize=(8, len(col_combs) * 5))\r\n",
    "    for jj, col_comb in enumerate(col_combs):\r\n",
    "        current_axis = ax[jj] if len(col_combs) > 1 else ax\r\n",
    "        clusters = list(range(2, 10))\r\n",
    "        sil = pd.DataFrame(data=clusters, columns=['Num'])\r\n",
    "        dists = []\r\n",
    "        silhoutte_scores = []\r\n",
    "        col_list = [*col_comb]\r\n",
    "        for i in clusters:\r\n",
    "            model = KMeans(n_clusters=i).fit(X=df[col_list])\r\n",
    "            y_hat = model.predict(X=df[col_list])\r\n",
    "            dists.append(get_dist_score(df, i))\r\n",
    "            silhoutte_scores.append(silhouette_score(X=df[col_list], labels=y_hat))\r\n",
    "        col_str = ', '.join(col_list)\r\n",
    "        sil = pd.concat(objs=(sil, pd.Series(\r\n",
    "            name=col_str,\r\n",
    "            data=silhoutte_scores\r\n",
    "        )), axis=1)\r\n",
    "        # print(sil.melt(id_vars='Num'))\r\n",
    "        sns.lineplot(data=sil.melt(id_vars='Num'), x='Num', y='value', hue='variable', ax=current_axis)\r\n",
    "        # print(np.array(dists)[:,0])\r\n",
    "        # sns.lineplot(x=clusters, y=np.array(dists)[:, 0], ax=current_axis.twinx(), color='tab:orange')\r\n",
    "        sns.lineplot(x=clusters, y=dists, ax=current_axis.twinx(), color='tab:green')\r\n",
    "        # sns.lineplot(x=clusters, y=np.array(dists)[:, 2], ax=current_axis.twinx(), color='tab:grey')\r\n",
    "        # ax.legend(bbox_to_anchor=(2, 1))\r\n",
    "\r\n",
    "get_silhouette_score_plot(df=df2, ignore_cols='Gender')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cluster_params = [4, 5]\r\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 8))\r\n",
    "for i, cl in enumerate(cluster_params):\r\n",
    "    model = KMeans(n_clusters=cl).fit(X=df2[['Annual Income (k$)', 'Spending Score (1-100)']])\r\n",
    "    y_hat = model.predict(df2[['Annual Income (k$)', 'Spending Score (1-100)']])\r\n",
    "    sns.scatterplot(data=df2, x='Annual Income (k$)', y='Spending Score (1-100)', hue=y_hat, ax=ax[i])\r\n",
    "    print(f'Silhouette Score ({cl}):', silhouette_score(X=df2[['Annual Income (k$)', 'Spending Score (1-100)']], labels=model.labels_, metric='euclidean'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import plotly.express as px\r\n",
    "from sklearn.cluster import DBSCAN, OPTICS, AgglomerativeClustering, AffinityPropagation\r\n",
    "\r\n",
    "for mo in [KMeans(n_clusters=6), DBSCAN(eps=15, min_samples=15), OPTICS(max_eps=18)]:\r\n",
    "    colrs = mo.fit_predict(X=df2[['Annual Income (k$)', 'Age', 'Spending Score (1-100)']])\r\n",
    "    fig = px.scatter_3d(data_frame=df2, x='Annual Income (k$)', y='Age', z='Spending Score (1-100)', color=colrs, title=type(mo).__name__ + ' ' + str(round(silhouette_score(X=df2[['Annual Income (k$)', 'Age', 'Spending Score (1-100)']], labels=colrs), 2)), color_continuous_scale=px.colors.sequential.Rainbow)\r\n",
    "    fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}