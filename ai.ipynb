{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit ('.venv': pipenv)"
  },
  "interpreter": {
   "hash": "b85dc5435e0bfe71fff45c7bd600e0d35505844357c06700d87c8785fb37e4d4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part A > Time Series Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Data Manipulation Dependencies\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Graphing Dependencies\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "# Miscellaneous Dependencies\r\n",
    "from typing import Union, List, Tuple, Dict, Callable\r\n",
    "import warnings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Hide all warnings\r\n",
    "warnings.filterwarnings(action='ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Time Series Dependencies\r\n",
    "import statsmodels\r\n",
    "import statsmodels.tsa as time_series_analysis\r\n",
    "from pmdarima.arima import arima\r\n",
    "from statsmodels.tsa.stattools import adfuller\r\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\r\n",
    "from statsmodels.tsa.arima.model import ARIMA\r\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\r\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('./data/train.csv')\r\n",
    "df['Date'] = pd.to_datetime(df['Date'])\r\n",
    "df.set_index(keys='Date', inplace=True)\r\n",
    "df.sort_index(inplace=True)\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df[df['Value'] > 0]\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.groupby(by=[df.index.year, df.index.quarter]).count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def groupby_date(df: pd.DataFrame, freq: str = 'D'):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    gbo = df_tmp.resample(rule=freq).mean()\r\n",
    "    return gbo"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# def drop_unreasonable_values(df: pd.DataFrame, mask: np.ndarray):\r\n",
    "#     print(df[mask].shape, df[~mask].shape)\r\n",
    "#     return df[mask]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def drop_sparse_data(df, threshold: int = 20, freq: str = 'days'):\r\n",
    "    from datetime import timedelta\r\n",
    "    index = df.index\r\n",
    "    x = 0\r\n",
    "    drop_rows = []\r\n",
    "    for i in df.index:\r\n",
    "        for n in range(1, threshold + 1):\r\n",
    "            if (i + timedelta(**{freq: n})) not in index:\r\n",
    "                drop_rows.append(i)\r\n",
    "                break\r\n",
    "        x += 1\r\n",
    "    return df.drop(drop_rows, axis=0)\r\n",
    "\r\n",
    "CO = df[df['Gas'] == 'CO']\r\n",
    "CO.groupby(by=[CO.index.year, CO.index.quarter]).count()\r\n",
    "print(f'''Dropped {CO.shape[0] - drop_sparse_data(CO, threshold=14).shape[0]} rows\r\n",
    "Last 5 Rows of CO:\r\n",
    "{drop_sparse_data(CO, threshold=14).iloc[-5:]}''')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def impute_missing_values(df: pd.DataFrame, method: Union[str, int] = 'time'):\r\n",
    "    if type(method) is int:\r\n",
    "        return df.rolling(window=method).mean()\r\n",
    "    else:\r\n",
    "        return df.interpolate(method=method)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_df_partitioned_by_gas(df: pd.DataFrame, resample_freq: str = 'D'):\r\n",
    "    gas_dict = {}\r\n",
    "    for gas in df['Gas'].unique():  # type: ignore\r\n",
    "        gbo = drop_sparse_data(df[df['Gas'] == gas], threshold=3)\r\n",
    "        gbo = groupby_date(df=gbo, freq=resample_freq)\r\n",
    "        # print(gbo['Value'].mean(), gbo['Value'].std())\r\n",
    "        dummy_date_range = pd.date_range(start=gbo.index[0], end=gbo.index[-1], freq=resample_freq)\r\n",
    "        dummy_date_frame = pd.DataFrame(index=dummy_date_range, data=[gbo[date]['Value'] if np.isin(date, gbo.index) else np.nan for date in dummy_date_range])\r\n",
    "        gbo = pd.merge(left=gbo, right=dummy_date_frame, left_index=True, right_index=True, how='right')\r\n",
    "        gbo = impute_missing_values(gbo.drop(columns=0))\r\n",
    "        gbo['Gas'] = [gas] * gbo.shape[0]\r\n",
    "        gas_dict[gas] = gbo\r\n",
    "    return gas_dict\r\n",
    "\r\n",
    "df_partitioned = get_df_partitioned_by_gas(df)\r\n",
    "df_partitioned['CO']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_general_trends(df_dict: Dict[str, pd.DataFrame], ma_windows: List[int] = [1, 7, 30]):\r\n",
    "    cells = len(df_dict.keys())\r\n",
    "    fig, ax = plt.subplots(nrows=cells // 2, ncols=cells // 2, figsize=(12, 8))\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        df = df_dict[gas]\r\n",
    "        for w in ma_windows:\r\n",
    "            df_tmp = df.rolling(window=w).mean()\r\n",
    "            ax[i // 2, i % 2].plot(df_tmp.index, df_tmp['Value'], label=f'{w}-day ma')\r\n",
    "    ax[0, 0].legend()\r\n",
    "\r\n",
    "get_general_trends(df_dict=df_partitioned, ma_windows=[30])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_plots_by_gas(df_dict: pd.DataFrame):\r\n",
    "    def get_plot(df: pd.DataFrame, ax):\r\n",
    "        ax.plot(df['T'], label='T')\r\n",
    "        ax.plot(df['RH'], label='RH')\r\n",
    "        ax.plot([], [], color='g', label='Value')\r\n",
    "\r\n",
    "        ax2 = ax.twinx()\r\n",
    "        ax2.plot(df['Value'], color='green')\r\n",
    "        # ax.set_xticklabels(labels=df.index.month)\r\n",
    "\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\r\n",
    "    gg = [(0, 0), (0, 1), (1, 0), (1, 1)]\r\n",
    "    \r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        current_ax = ax[(i // 2, i % 2)]\r\n",
    "        df_tmp = df_dict[gas]\r\n",
    "        get_plot(df_tmp, ax=current_ax)\r\n",
    "        current_ax.set_title(gas)\r\n",
    "        cc = current_ax.get_xticks()\r\n",
    "        # current_ax.set_xticklabels(tick_labels=pd.to_datetime(current_ax.get_xticks()).year())\r\n",
    "    \r\n",
    "    ax[0, 0].legend()\r\n",
    "    fig.tight_layout()\r\n",
    "\r\n",
    "get_plots_by_gas(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_gas_distributions(df: pd.DataFrame):\r\n",
    "    counts = df.groupby(by='Gas').count().median(axis=1).astype(int)\r\n",
    "    sns.barplot(x=counts.index, y=counts, palette='deep')\r\n",
    "\r\n",
    "get_gas_distributions(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_gas_means_and_medians(df):\r\n",
    "    # fig, ax = plt.subplots(nrows=2, ncols=2)\r\n",
    "    gb = df.groupby(by='Gas', as_index=False)\r\n",
    "    mean_ = gb.mean()\r\n",
    "    mean_['Type'] = ['Mean'] * mean_.shape[0]\r\n",
    "    types = ['Mean', 'Median']\r\n",
    "    comb_df = pd.DataFrame()\r\n",
    "    for i, frame in enumerate([gb.mean(), gb.median()]):\r\n",
    "        tmp_df = frame\r\n",
    "        tmp_df['Type'] = types[i]\r\n",
    "        comb_df = pd.concat(objs=(comb_df, tmp_df), axis=0)\r\n",
    "    # print(comb_df)\r\n",
    "    sns.barplot(data=comb_df, x='Gas', y='Value', hue='Type', palette='rainbow')\r\n",
    "get_gas_means_and_medians(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from statsmodels.tsa.stattools import pacf, acf\r\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\r\n",
    "\r\n",
    "def cf_summary(df_dict: Dict[str, pd.DataFrame], kind: Union['pacf', 'acf'], resample: str = 'M', threshold: float = 0.01, plot: bool = False):\r\n",
    "    corr_fn = pacf if kind == 'pacf' else acf\r\n",
    "    plot_fn = plot_pacf if kind == 'pacf' else plot_acf\r\n",
    "    gases = []\r\n",
    "    best_ts = []\r\n",
    "    t_values_2 = []\r\n",
    "    for gas in df_dict.keys():\r\n",
    "        current_gas = gas\r\n",
    "        df = df_dict[gas]\r\n",
    "        gases.append(current_gas)\r\n",
    "        lag_space = min(df.count()[0] // 2 - 1, 50)\r\n",
    "        # lag_space = 20\r\n",
    "        corr_scores, conf_intvs = corr_fn(df['Value'], nlags=lag_space, alpha=threshold)\r\n",
    "        lower_conf_bound = conf_intvs[:, 0] - corr_scores\r\n",
    "        upper_conf_bound = conf_intvs[:, 1] - corr_scores\r\n",
    "        t_values = np.where((corr_scores < lower_conf_bound) | (corr_scores > upper_conf_bound))[0][1:]\r\n",
    "        best_t = t_values[np.argmax((np.abs(corr_scores[t_values])))] if len(t_values) > 0 else 0\r\n",
    "        best_ts.append(best_t)\r\n",
    "        t_values_2.append(t_values)\r\n",
    "        if plot:\r\n",
    "            plot_fn(df['Value'], lags=lag_space, alpha=threshold).get_axes()[0].set_title(f'{current_gas}: possible t-values: {tuple(t_values)} Best t: {best_t}')\r\n",
    "    t_dict = {}\r\n",
    "    for i, gas in enumerate(gases):\r\n",
    "        t_dict[gas] = t_values_2[i]\r\n",
    "    return t_dict, pd.DataFrame(data={\r\n",
    "        'Gas': gases,\r\n",
    "        't': best_ts\r\n",
    "    }).set_index(keys='Gas')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cf_summary_ind(df: pd.DataFrame, kind: Union['pacf', 'acf'], resample: str = 'M', threshold: float = 0.01, plot: bool = False):\r\n",
    "    corr_fn = pacf if kind == 'pacf' else acf\r\n",
    "    plot_fn = plot_pacf if kind == 'pacf' else plot_acf\r\n",
    "    # lag_space = df.count() // 2- 1\r\n",
    "    lag_space = min(12, df.count() // 2 - 1)\r\n",
    "    corr_scores, conf_intvs = corr_fn(df, nlags=lag_space, alpha=threshold)\r\n",
    "    lower_conf_bound = conf_intvs[:, 0] - corr_scores\r\n",
    "    upper_conf_bound = conf_intvs[:, 1] - corr_scores\r\n",
    "    t_values = np.where((corr_scores < lower_conf_bound) | (corr_scores > upper_conf_bound))[0][1:]\r\n",
    "    best_t = t_values[np.argmax((np.abs(corr_scores[t_values])))] if len(t_values) > 0 else 0\r\n",
    "    if plot:\r\n",
    "        plot_fn(df, lags=lag_space, alpha=threshold).get_axes()[0].set_title(f'possible t-values: {tuple(t_values)} Best t: {best_t}')\r\n",
    "    t_dict = {}\r\n",
    "    return t_values, best_t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_stationarity_summary(df_dict: Dict[str, pd.DataFrame], plot: bool = False):\r\n",
    "    def get_stationarity_of_columns(df: pd.DataFrame, gas: str, significance_level: float = 0.05):\r\n",
    "        p_value = round(adfuller(df['Value'])[1], 5)\r\n",
    "        # if p_value < significance_level:\r\n",
    "        #     print(f'Stationary ({p_value})')\r\n",
    "        # else:\r\n",
    "        #     print(f'Non-Stationary ({p_value})')\r\n",
    "        decomposition = seasonal_decompose(df['Value'])\r\n",
    "        p_values, best_p = cf_summary_ind(decomposition.seasonal, kind='pacf')\r\n",
    "        q_values, best_q = cf_summary_ind(decomposition.seasonal, kind='acf')\r\n",
    "        if plot:\r\n",
    "            ax = decomposition.plot().get_axes()[0]\r\n",
    "            ax.set_title(gas)\r\n",
    "        return p_value < significance_level, p_value, p_values, best_p, q_values, best_q\r\n",
    "\r\n",
    "    stationarity_dict = {}\r\n",
    "    for gas in df_dict.keys():\r\n",
    "        stationary, p_value, p_values, best_p, q_values, best_q = get_stationarity_of_columns(df_dict[gas], gas)\r\n",
    "        stationarity_dict[gas] = {\r\n",
    "            'stationary': stationary,\r\n",
    "            'stationarity p': p_value,\r\n",
    "            'p_values': p_values,\r\n",
    "            'best_p': best_p,\r\n",
    "            'q_values': q_values,\r\n",
    "            'best_q': best_q\r\n",
    "        }\r\n",
    "    return stationarity_dict\r\n",
    "\r\n",
    "stats = get_stationarity_summary(df_partitioned)\r\n",
    "stats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# baseline prediction, since they are all stationary\r\n",
    "test_set = pd.read_csv(filepath_or_buffer='./data/test.csv', sep=',')\r\n",
    "pd.merge(left=test_set, right=df.groupby(by='Gas').mean(), left_on='Gas', right_index=True, how='inner')[['id', 'Value']].set_index(keys='id').to_csv('./out/using_mean.csv', sep=',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_long_term_trend(df_dict: Dict[str, pd.DataFrame]):\r\n",
    "    value_dict = {}\r\n",
    "    for gas in df_dict.keys():\r\n",
    "        df = df_dict[gas].rolling(window=30).mean()['Value'].dropna(axis=0)\r\n",
    "        stationary = adfuller(df)[1] < 0.05\r\n",
    "        p_values, best_p = cf_summary_ind(df, kind='pacf', threshold=0.05)\r\n",
    "        q_values, best_q = cf_summary_ind(df, kind='acf', threshold=0.05)\r\n",
    "        value_dict[gas] = {\r\n",
    "            'p': best_p,\r\n",
    "            'q': best_q,\r\n",
    "            'stationary': stationary\r\n",
    "        }\r\n",
    "    return value_dict\r\n",
    "\r\n",
    "get_long_term_trend(df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def combine(df_dict: Dict[str, pd.DataFrame]):\r\n",
    "    stationarity_summary = get_stationarity_summary(df_dict)\r\n",
    "    lt_stationarity_summary = get_long_term_trend(df_dict)\r\n",
    "    orders = {}\r\n",
    "    # m_values = (12, 12, 2, 4)\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        p = stationarity_summary[gas]['best_p']\r\n",
    "        d = 0 if stationarity_summary[gas]['stationary'] else 1\r\n",
    "        q = stationarity_summary[gas]['best_q']\r\n",
    "\r\n",
    "        P = lt_stationarity_summary[gas]['p']\r\n",
    "        D = 0 if lt_stationarity_summary[gas]['stationary'] else 1\r\n",
    "        Q = lt_stationarity_summary[gas]['q']\r\n",
    "\r\n",
    "        # m = m_values[i]\r\n",
    "        m = 12\r\n",
    "        orders[gas] = ((p, d, q), (P, D, Q, m))\r\n",
    "    return orders\r\n",
    "\r\n",
    "combine(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "def finale(df_dict: Dict[str, pd.DataFrame], test_size: float = 0.75):\r\n",
    "    orders = combine(df_dict=df_dict)\r\n",
    "    # print(orders)\r\n",
    "    models = {}\r\n",
    "    for gas in df_dict.keys():\r\n",
    "\r\n",
    "        data = df_dict[gas]['Value']\r\n",
    "        partition_index = int(data.shape[0] * test_size)\r\n",
    "\r\n",
    "        train = data[:partition_index]\r\n",
    "        test = data[partition_index:]\r\n",
    "\r\n",
    "        model = SARIMAX(train, order=orders[gas][0], seasonal_order=orders[gas][1]).fit()\r\n",
    "        \r\n",
    "        train_pred = model.predict(start=0, end=partition_index - 1)\r\n",
    "        test_pred = model.forecast(steps=data.shape[0] - partition_index)\r\n",
    "\r\n",
    "        train_err = mean_squared_error(train, train_pred, squared=False)\r\n",
    "        test_err = mean_squared_error(test, test_pred, squared=False)\r\n",
    "\r\n",
    "        models[gas] = {\r\n",
    "            'train_true': train,\r\n",
    "            'train_pred': train_pred,\r\n",
    "            'train_rmse': train_err,\r\n",
    "            'test_true': test,\r\n",
    "            'test_pred': test_pred,\r\n",
    "            'test_rmse': test_err\r\n",
    "        }\r\n",
    "    return models\r\n",
    "\r\n",
    "gg = finale(df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_test_df(file_path: str = './data/test.csv'):\r\n",
    "    test_df = pd.read_csv(filepath_or_buffer=file_path, sep=',', header=0)\r\n",
    "    test_df['Date'] = pd.to_datetime(test_df['Date'], format='%d/%m/%Y')\r\n",
    "    test_df.set_index(keys='Date', inplace=True)\r\n",
    "    return test_df.sort_index(ascending=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def kaggle_submission(df_dict: Dict[str, pd.DataFrame], test_file: str, submission_file: str):\r\n",
    "    orders = combine(df_dict)\r\n",
    "    test_df = pd.read_csv(filepath_or_buffer=test_file, sep=',', header=0)\r\n",
    "    # print(test_df.head())\r\n",
    "    result_set = {}\r\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\r\n",
    "    with warnings.catch_warnings():\r\n",
    "        warnings.simplefilter('ignore', ConvergenceWarning)\r\n",
    "        for gas in df_dict.keys():\r\n",
    "            df = df_dict[gas]['Value']\r\n",
    "            target = test_df[test_df['Gas'] == gas]\r\n",
    "            target['Date'] = pd.to_datetime(target['Date'], format='%d/%m/%Y')\r\n",
    "            target.set_index(keys='Date', inplace=True)\r\n",
    "            target.sort_index(inplace=True)\r\n",
    "            model = SARIMAX(df, order=orders[gas][0], seasonal_order=orders[gas][1]).fit()\r\n",
    "            pred = model.forecast(target.index[-1]).rename('Value')\r\n",
    "            result = pd.merge(left=pred, right=target, left_index=True, right_index=True, how='inner')\r\n",
    "            print(result.shape)\r\n",
    "            result_set[gas] = result[['id', 'Value']].set_index(keys='id')\r\n",
    "\r\n",
    "    pred_df = pd.DataFrame(columns=result_set['CO'].columns)\r\n",
    "    for gas in result_set.keys():\r\n",
    "        pred_df = pd.concat(objs=(pred_df, result_set[gas]), axis=0)\r\n",
    "    \r\n",
    "    return pred_df.sort_index(ascending=True)\r\n",
    "\r\n",
    "pred_df = kaggle_submission(df_partitioned, './data/test.csv', '')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pred_df.to_csv('./out/base-ii.csv', sep=',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error\r\n",
    "def evaluate_predictions(pred_df: pd.DataFrame):\r\n",
    "    # plt.plot(gg['CO']['train_true'])\r\n",
    "    # plt.plot(gg['CO']['test_true'])\r\n",
    "    # plt.plot(gg['CO']['train_pred'])\r\n",
    "    # plt.plot(gg['CO']['test_pred'])\r\n",
    "    # print(sum(list(map(lambda x: (gg[x]['train_rmse'] + gg[x]['test_rmse']) / 2, gg))) / 4)\r\n",
    "    # for gas in gg.keys():\r\n",
    "    #     print(gg[gas]['train_rmse'], gg[gas]['test_rmse'])\r\n",
    "    target = get_test_df()\r\n",
    "    df = target[target['Gas'] == 'CO']['id']\r\n",
    "    comb = pd.merge(left=df, right=pred_df, left_on='id', right_index=True, how='inner')\r\n",
    "    plt.plot(comb['Value'])\r\n",
    "    plt.plot(df_partitioned['CO']['Value'])\r\n",
    "\r\n",
    "evaluate_predictions(pred_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#differencing\r\n",
    "# from statsmodels.tsa.statespace.tools import diff\r\n",
    "\r\n",
    "# def get_diff(df):\r\n",
    "#     return df['Value'], diff(df['Value'])\r\n",
    "\r\n",
    "# first, second = get_diff(df_partitioned['CO'])\r\n",
    "\r\n",
    "# plt.plot(first.index, first.values)\r\n",
    "# plt.plot(second.index, second.values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pacf_dict, pacf_summary = cf_summary(df_partitioned, kind='pacf', threshold=0.05)\r\n",
    "print(pacf_dict, pacf_summary)\r\n",
    "\r\n",
    "acf_dict, acf_summary = cf_summary(df_partitioned, kind='acf', threshold=0.05)\r\n",
    "print(acf_dict, acf_summary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Loop thru gases, thresholds and freqs to get dataframe (sth like gridsearch but for time series)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# def arima(df: pd.DataFrame, order: Tuple[int, int, int], print_summary: bool = False):\r\n",
    "#     model = time_series_analysis.arima.model.ARIMA(df['Value'], order=order)\r\n",
    "#     res = model.fit()\r\n",
    "#     if print_summary:\r\n",
    "#         print(res.summary())\r\n",
    "#     return res\r\n",
    "\r\n",
    "# arima(df=df_partitioned['CO'], order=(1, 0, 1)).aic"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# def grid_search(\r\n",
    "#     df: pd.Series,\r\n",
    "#     p_space: List[int] = [1],\r\n",
    "#     d: int = 0,\r\n",
    "#     q_space: List[int] = [1],\r\n",
    "#     P_space: List[int] = None,\r\n",
    "#     D: int = None,\r\n",
    "#     Q_space: List[int] = None,\r\n",
    "#     m_space: List[int] = [12],\r\n",
    "#     seasonal: bool = False,\r\n",
    "#     use_analogue: bool = True\r\n",
    "# ):\r\n",
    "#     from itertools import product\r\n",
    "#     if seasonal:\r\n",
    "#         if use_analogue:\r\n",
    "#             P_space = p_space\r\n",
    "#             D = d\r\n",
    "#             Q_space = q_space\r\n",
    "#         else:\r\n",
    "#             P_space = p_space if P_space is None else P_space\r\n",
    "#             D = d if D is None else D\r\n",
    "#             Q_space = q_space if Q_space is None else Q_space\r\n",
    "#         for m in m_space:\r\n",
    "#             for order in product(p_space, [d], q_space):\r\n",
    "#                 for seasonal_order in product(P_space, [D], Q_space):\r\n",
    "#                     p, d, q = order\r\n",
    "#                     P, D, Q = seasonal_order\r\n",
    "#                     print((p, d, q), (P, D, Q, m))\r\n",
    "#                     model = SARIMAX(endog=df, order=(p, d, q), seasonal_order=(P, D, Q, m)).fit()\r\n",
    "#                     print(model.aic)\r\n",
    "\r\n",
    "# grid_search(df_partitioned['CO']['Value'],\r\n",
    "#     p_space=[32, 38],\r\n",
    "#     q_space=[3, 5, 7],\r\n",
    "#     m_space=[8],\r\n",
    "#     P_space=[],\r\n",
    "#     seasonal=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(train)\r\n",
    "plt.plot(test)\r\n",
    "plt.plot(final.predict(start=0, end=int(history_length * 0.75)))\r\n",
    "plt.plot(final.forecast(steps=target.shape[1] - int(history_length * 0.75) - 1))\r\n",
    "mean_squared_error(train, final.predict(start=1, end=int(history_length * 0.75)), squared=False), mean_squared_error(test, final.forecast(steps=target.shape[1] - int(history_length * 0.75) - 1), squared=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\r\n",
    "\r\n",
    "target = df_partitioned['CO']['Value']\r\n",
    "history_length = target.shape[0]\r\n",
    "train = target.iloc[:int(history_length * 0.75)]\r\n",
    "test = target.iloc[int(history_length * 0.75):]\r\n",
    "\r\n",
    "pickle.dump(obj=1, file=open('./tmp/stuff.p', 'wb'))\r\n",
    "# final = SARIMAX(train, order=(1, 0, 1), seasonal_order=(5, 1, 0, 8)).fit()\r\n",
    "print(final.summary())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# def test_facility(df_dict: Dict[str, pd.DataFrame], gas: str, order=Tuple[int, int, int]):\r\n",
    "#     global pacf_dict, acf_dict\r\n",
    "#     df = df_dict[gas]\r\n",
    "#     fig, ax = plt.subplots(nrows=len(pacf_dict[gas]) * len(acf_dict[gas]), figsize=(len(pacf_dict[gas]) * len(acf_dict[gas]) * 5, 8))\r\n",
    "#     i = 0\r\n",
    "#     for p in pacf_dict[gas]:\r\n",
    "#         for q in acf_dict[gas]:\r\n",
    "#             model = arima(df=df, order=(p, order[1], q))\r\n",
    "#             first_date = model.predict(start=0, end=0).index[0]\r\n",
    "#             y_hat = model.predict(start=1, end=300)\r\n",
    "#             y_hat.index = pd.date_range(start=first_date, periods=300, freq='D')\r\n",
    "#             df_tmp = pd.DataFrame(data={\r\n",
    "#                 'Value': y_hat,\r\n",
    "#                 'Color': np.isin(y_hat.index, df.index)\r\n",
    "#             })\r\n",
    "#             hv = df_tmp[df_tmp['Color']]\r\n",
    "#             donthv = df_tmp[~df_tmp['Color']]\r\n",
    "#             # bridge = df_tmp[hv.index[-1]:donthv.index[0]]\r\n",
    "#             sns.lineplot(x=hv.index, y=hv['Value'], color='blue', ax=ax[i])\r\n",
    "#             # sns.lineplot(x=bridge.index, y=bridge['Value'], color='green', ax=ax)\r\n",
    "#             sns.lineplot(x=donthv.index, y=donthv['Value'], color='green', ax=ax[i])\r\n",
    "#             sns.lineplot(x=hv.index, y=df['Value'], color='orange', ax=ax[i])\r\n",
    "#             aic = model.aic\r\n",
    "#             print(p, q, aic)\r\n",
    "#             i += 1\r\n",
    "\r\n",
    "#     # print(r2_score(df[hv.index], hv['Value']))\r\n",
    "\r\n",
    "# test_facility(df_partitioned, 'NMHC', (0, 0, 0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from pmdarima.arima import auto_arima\r\n",
    "\r\n",
    "# def auto(df: pd.DataFrame):\r\n",
    "#     arima_model = auto_arima(df['Value'], seasonal=True, m=8)\r\n",
    "#     print(arima_model.summary())\r\n",
    "\r\n",
    "# auto(df_partitioned['CO'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from statsmodels.tsa.holtwinters import Holt\r\n",
    "# from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "# model = Holt(df_partitioned['CO']['Value']).fit()\r\n",
    "# y_hat_b4 = model.forecast(steps=100)\r\n",
    "\r\n",
    "# y_true = df_partitioned['CO']['Value']\r\n",
    "# model = ExponentialSmoothing(endog=df_partitioned['CO']['Value'], seasonal='mul', seasonal_periods=2).fit()\r\n",
    "# y_hat = model.predict(start=0, end=391)\r\n",
    "# y_pred = model.forecast(steps=50)\r\n",
    "# plt.plot(y_true)\r\n",
    "# plt.plot(y_hat)\r\n",
    "# plt.plot(y_pred)\r\n",
    "# model.aic, mean_squared_error(y_true, y_hat, squared=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# use supervised learning to check\r\n",
    "# use varima to check correlation between T and RH"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "\r\n",
    "pca = PCA(n_components=3).fit(StandardScaler().fit_transform(df[['T', 'RH']]))\r\n",
    "pca.components_\r\n",
    "# df[['T', 'RH', 'Value']].corr()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part B > Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Exclusive Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2 = pd.read_csv('./data/Mall_Customers.csv', index_col=0)\r\n",
    "df2.rename(mapper={'Genre': 'Gender'}, axis=1, inplace=True)\r\n",
    "df2.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "['Eigenvalue', 'Explained Variance', 'Cumulative Explained Variance'].extend(df2.drop(columns='Gender').columns.tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from typing import Union, List\r\n",
    "\r\n",
    "def get_pca_results(df: pd.DataFrame, ignore_cols: Union[str, List[str]]):\r\n",
    "    df_scaled = StandardScaler().fit_transform(X=df.drop(columns=ignore_cols))\r\n",
    "\r\n",
    "    pca = PCA(n_components=df_scaled.shape[1]).fit(X=df_scaled)\r\n",
    "    header = ['Eigenvalue', 'Explained Variance', 'Cumulative Explained Variance']\r\n",
    "    header.extend(df.drop(columns=ignore_cols).columns.tolist())\r\n",
    "    eigenvalues = pca.explained_variance_\r\n",
    "    eigenvectors = pca.components_\r\n",
    "    expl_var = pca.explained_variance_ratio_\r\n",
    "    cum_expl_var = pca.explained_variance_ratio_.cumsum()\r\n",
    "    pca_results = pd.DataFrame(\r\n",
    "        data=np.hstack((\r\n",
    "            eigenvalues.reshape(-1, 1),\r\n",
    "            expl_var.reshape(-1, 1),\r\n",
    "            cum_expl_var.reshape(-1, 1),\r\n",
    "            eigenvectors\r\n",
    "        )),\r\n",
    "        columns=header,\r\n",
    "        index=[f'PC {i + 1}' for i in range(df_scaled.shape[1])]\r\n",
    "    )\r\n",
    "\r\n",
    "    df_transformed = pd.DataFrame(\r\n",
    "        data=pca.transform(df_scaled),\r\n",
    "        index=df.index,\r\n",
    "        columns=[f'PC {i + 1}' for i in range(df_scaled.shape[1])]\r\n",
    "    )\r\n",
    "\r\n",
    "    return pca_results, df_transformed\r\n",
    "\r\n",
    "pca_results, df2_transformed = get_pca_results(df=df2, ignore_cols='Gender')\r\n",
    "pca_results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scree_plot(df: pd.DataFrame, pca: pd.DataFrame):\r\n",
    "    with sns.axes_style(style='darkgrid'):\r\n",
    "        ax = sns.pointplot(data=pca, x=pca.index, y=pca['Eigenvalue'])\r\n",
    "        ax.set(\r\n",
    "            title='Scree Plot for PCA (df2)',\r\n",
    "            ylim=(0, 1.4)\r\n",
    "        )\r\n",
    "        ax.annotate(text='As there is no elbow,\\nno PC should be discarded', xy=(1.75, 1.2), ha='center')\r\n",
    "        return ax\r\n",
    "\r\n",
    "scree_plot(df2, pca_results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_dist_score(df: pd.DataFrame, clusters: int):\r\n",
    "    model = KMeans(n_clusters=clusters).fit(X=df[['Annual Income (k$)', 'Spending Score (1-100)']])\r\n",
    "    centers = model.cluster_centers_\r\n",
    "    y_hat = model.predict(df[['Annual Income (k$)', 'Spending Score (1-100)']])\r\n",
    "    # plt.scatter(centers[:,0], centers[:,1])\r\n",
    "    # sns.scatterplot(x=df['Annual Income (k$)'], y=df['Spending Score (1-100)'], hue=y_hat)\r\n",
    "    df_tmp = pd.concat(objs=(df, pd.Series(data=y_hat, name='Class', index=df.index)), axis=1)\r\n",
    "    means = []\r\n",
    "    stds = []\r\n",
    "    count = []\r\n",
    "    for i, c in enumerate(centers):\r\n",
    "        # plt.annotate(text=str(model.predict(c.reshape(-1, 2))), xy=(c[0], c[1])) # type: ignore\r\n",
    "        class_i = df_tmp[df_tmp['Class'] == i][['Annual Income (k$)', 'Spending Score (1-100)']]\r\n",
    "        distances = np.linalg.norm(class_i - centers[i, :], axis=1)\r\n",
    "        # print(i, int(class_i.count(axis=0).mean()), distances.mean(), distances.std())\r\n",
    "        # count.append(int(class_i.count(axis=0).mean()))\r\n",
    "        means.append(distances.mean())\r\n",
    "        # stds.append(distances.std())\r\n",
    "    return np.std(means)\r\n",
    "\r\n",
    "get_dist_score(df2, 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# type: ignore\r\n",
    "from sklearn.metrics import silhouette_score\r\n",
    "from more_itertools import powerset\r\n",
    "\r\n",
    "def get_silhouette_score_plot(df: pd.DataFrame, ignore_cols: Union[str, List[str]] = None):\r\n",
    "    ignore_cols = ignore_cols if ignore_cols is not None else [] \r\n",
    "    col_combs = list(filter(lambda x: len(x) > 1, list(powerset(df.drop(columns=ignore_cols).columns))))\r\n",
    "    # n = len(cols)\r\n",
    "    # n = 10\r\n",
    "    # fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(10, 8))\r\n",
    "    # c = 0\r\n",
    "    fig, ax = plt.subplots(nrows=len(col_combs), figsize=(8, len(col_combs) * 5))\r\n",
    "    for jj, col_comb in enumerate(col_combs):\r\n",
    "        current_axis = ax[jj] if len(col_combs) > 1 else ax\r\n",
    "        clusters = list(range(2, 10))\r\n",
    "        sil = pd.DataFrame(data=clusters, columns=['Num'])\r\n",
    "        dists = []\r\n",
    "        silhoutte_scores = []\r\n",
    "        col_list = [*col_comb]\r\n",
    "        for i in clusters:\r\n",
    "            model = KMeans(n_clusters=i).fit(X=df[col_list])\r\n",
    "            y_hat = model.predict(X=df[col_list])\r\n",
    "            dists.append(get_dist_score(df, i))\r\n",
    "            silhoutte_scores.append(silhouette_score(X=df[col_list], labels=y_hat))\r\n",
    "        col_str = ', '.join(col_list)\r\n",
    "        sil = pd.concat(objs=(sil, pd.Series(\r\n",
    "            name=col_str,\r\n",
    "            data=silhoutte_scores\r\n",
    "        )), axis=1)\r\n",
    "        # print(sil.melt(id_vars='Num'))\r\n",
    "        sns.lineplot(data=sil.melt(id_vars='Num'), x='Num', y='value', hue='variable', ax=current_axis)\r\n",
    "        # print(np.array(dists)[:,0])\r\n",
    "        # sns.lineplot(x=clusters, y=np.array(dists)[:, 0], ax=current_axis.twinx(), color='tab:orange')\r\n",
    "        sns.lineplot(x=clusters, y=dists, ax=current_axis.twinx(), color='tab:green')\r\n",
    "        # sns.lineplot(x=clusters, y=np.array(dists)[:, 2], ax=current_axis.twinx(), color='tab:grey')\r\n",
    "        # ax.legend(bbox_to_anchor=(2, 1))\r\n",
    "\r\n",
    "get_silhouette_score_plot(df=df2, ignore_cols='Gender')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cluster_params = [4, 5]\r\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 8))\r\n",
    "for i, cl in enumerate(cluster_params):\r\n",
    "    model = KMeans(n_clusters=cl).fit(X=df2[['Annual Income (k$)', 'Spending Score (1-100)']])\r\n",
    "    y_hat = model.predict(df2[['Annual Income (k$)', 'Spending Score (1-100)']])\r\n",
    "    sns.scatterplot(data=df2, x='Annual Income (k$)', y='Spending Score (1-100)', hue=y_hat, ax=ax[i])\r\n",
    "    print(f'Silhouette Score ({cl}):', silhouette_score(X=df2[['Annual Income (k$)', 'Spending Score (1-100)']], labels=model.labels_, metric='euclidean'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import plotly.express as px\r\n",
    "from sklearn.cluster import DBSCAN, OPTICS, AgglomerativeClustering, AffinityPropagation\r\n",
    "\r\n",
    "for mo in [KMeans(n_clusters=6), DBSCAN(eps=15, min_samples=15), OPTICS(max_eps=18)]:\r\n",
    "    colrs = mo.fit_predict(X=df2[['Annual Income (k$)', 'Age', 'Spending Score (1-100)']])\r\n",
    "    fig = px.scatter_3d(data_frame=df2, x='Annual Income (k$)', y='Age', z='Spending Score (1-100)', color=colrs, title=type(mo).__name__ + ' ' + str(round(silhouette_score(X=df2[['Annual Income (k$)', 'Age', 'Spending Score (1-100)']], labels=colrs), 2)), color_continuous_scale=px.colors.sequential.Rainbow)\r\n",
    "    fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}