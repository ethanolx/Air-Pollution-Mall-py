{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit ('.venv': pipenv)"
  },
  "interpreter": {
   "hash": "b85dc5435e0bfe71fff45c7bd600e0d35505844357c06700d87c8785fb37e4d4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AIML CA2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import General Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Data Manipulation Dependencies\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Graphing Dependencies\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import plotly.express as px\r\n",
    "\r\n",
    "# Miscellaneous Dependencies\r\n",
    "from typing import Union, List, Tuple, Dict, Callable\r\n",
    "import warnings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Hide all warnings\r\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\r\n",
    "\r\n",
    "warnings.filterwarnings(action='ignore')\r\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part A > Time Series Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Exclusive Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Time Series Tools\r\n",
    "from statsmodels.tsa.stattools import adfuller\r\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\r\n",
    "\r\n",
    "# Time Series Models\r\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\r\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('./data/train.csv')\r\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\r\n",
    "df.set_index(keys='Date', inplace=True)\r\n",
    "df.sort_index(inplace=True)\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.groupby(by=[df.index.year, df.index.quarter]).count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def groupby_date(df: pd.DataFrame, freq: str = 'D'):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    gbo = df_tmp.resample(rule=freq).mean()\r\n",
    "    return gbo"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# def drop_unreasonable_values(df: pd.DataFrame, gas: str):\r\n",
    "#     mask = df['Value'] > 0\r\n",
    "#     df_tmp = df[mask]\r\n",
    "#     dropped_rows = df[~mask]\r\n",
    "#     print(f'Dropped {dropped_rows.shape[0]} rows for {gas} (unreasonable)')\r\n",
    "#     return df_tmp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def drop_outliers(df: pd.DataFrame, gas: str, band: float = 1.5):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    for c in ['T', 'RH', 'Value']:\r\n",
    "        vals = df[c].values\r\n",
    "        Q1 = np.quantile(vals, q=0.25)\r\n",
    "        Q3 = np.quantile(vals, q=0.75)\r\n",
    "        IQR = Q3 - Q1\r\n",
    "        df_tmp[(df[c] < (Q1 - band * IQR)) | (df[c] > (Q3 + band * IQR))] = np.nan\r\n",
    "        print(f'Dropped {df_tmp[(df[c] < (Q1 - band * IQR)) | (df[c] > (Q3 + band * IQR))].shape[0]} values for {gas} {c} (outlier)')\r\n",
    "    return df_tmp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# def drop_sparse_data(df, gas: str, threshold: int = 20, freq: str = 'days'):\r\n",
    "#     from datetime import timedelta\r\n",
    "#     index = df.index\r\n",
    "#     x = 0\r\n",
    "#     drop_rows = []\r\n",
    "#     for i in df.index[:-threshold]:\r\n",
    "#         for n in range(1, threshold + 1):\r\n",
    "#             if (i + timedelta(**{freq: n})) not in index:\r\n",
    "#                 drop_rows.append(i)\r\n",
    "#                 break\r\n",
    "#         x += 1\r\n",
    "#     print(f'Dropped {len(drop_rows)} rows for {gas} (sparse)')\r\n",
    "#     return df.drop(drop_rows, axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# def fill_missing_dates(df: pd.DataFrame, freq: str = 'D'):\r\n",
    "#     dummy_date_range = pd.date_range(start=df.index[0], end=df.index[-1], freq=freq)\r\n",
    "#     dummy_date_frame = pd.DataFrame(index=dummy_date_range, data=[df[date]['Value'] if np.isin(date, df.index) else np.nan for date in dummy_date_range])\r\n",
    "#     filled_df = pd.merge(left=df, right=dummy_date_frame, left_index=True, right_index=True, how='right').drop(columns=0)\r\n",
    "#     print(f'Inserted {filled_df.shape[0] - df.shape[0]} {df.shape} rows (filled)')\r\n",
    "#     return filled_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def impute_missing_values(df: pd.DataFrame, method: str = 'time'):\r\n",
    "    return df.interpolate(method=method)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_df_partitioned_by_gas(df: pd.DataFrame, resample_freq: str = 'D', clean: bool = True):\r\n",
    "    gas_dict = {}\r\n",
    "    for gas in df['Gas'].unique():\r\n",
    "        df_tmp = df[df['Gas'] == gas]\r\n",
    "        if clean:\r\n",
    "            df_tmp = drop_outliers(df_tmp, gas)\r\n",
    "            df_tmp = impute_missing_values(df_tmp)\r\n",
    "        df_tmp['Gas'] = [gas] * df_tmp.shape[0]\r\n",
    "        gas_dict[gas] = df_tmp\r\n",
    "    return gas_dict\r\n",
    "\r\n",
    "df_partitioned = get_df_partitioned_by_gas(df, clean=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Before Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_gas_distributions(df: pd.DataFrame):\r\n",
    "    counts = df.groupby(by='Gas').count().median(axis=1).astype(int)\r\n",
    "    sns.barplot(x=counts.index, y=counts, palette='deep')\r\n",
    "\r\n",
    "get_gas_distributions(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_gas_means_and_medians(df):\r\n",
    "    # fig, ax = plt.subplots(nrows=2, ncols=2)\r\n",
    "    gb = df.groupby(by='Gas', as_index=False)\r\n",
    "    mean_ = gb.mean()\r\n",
    "    mean_['Type'] = ['Mean'] * mean_.shape[0]\r\n",
    "    types = ['Mean', 'Median']\r\n",
    "    comb_df = pd.DataFrame()\r\n",
    "    for i, frame in enumerate([gb.mean(), gb.median()]):\r\n",
    "        tmp_df = frame\r\n",
    "        tmp_df['Type'] = types[i]\r\n",
    "        comb_df = pd.concat(objs=(comb_df, tmp_df), axis=0)\r\n",
    "    # print(comb_df)\r\n",
    "    sns.barplot(data=comb_df, x='Gas', y='Value', hue='Type', palette='rainbow')\r\n",
    "\r\n",
    "get_gas_means_and_medians(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_exog_variable_distributions(df: pd.DataFrame):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    df_tmp.resample(rule='D').mean()[['T', 'RH']]\r\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\r\n",
    "    sns.histplot(x=df_tmp['T'], ax=ax[0])\r\n",
    "    sns.histplot(x=df_tmp['RH'], ax=ax[1])\r\n",
    "\r\n",
    "get_exog_variable_distributions(df_partitioned['CO'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_gas_value_distributions(df_dict: Dict[str, pd.DataFrame]):\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2)\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        current_ax = ax[i // 2, i % 2]\r\n",
    "        current_ax.set_title(gas)\r\n",
    "        sns.histplot(data=df_dict[gas]['Value'], kde=True, ax=current_ax)\r\n",
    "\r\n",
    "get_gas_value_distributions(df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_general_trends(df_dict: Dict[str, pd.DataFrame], ma_windows: List[int] = [1, 7, 30], kind: str = 'group'):\r\n",
    "    cells = len(df_dict.keys())\r\n",
    "    fig, ax = plt.subplots(nrows=cells // 2, ncols=cells // 2, figsize=(12, 8))\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        df = df_dict[gas]\r\n",
    "        for w in ma_windows:\r\n",
    "            df_tmp = df.rolling(window=w).mean() if kind == 'ma' else groupby_date(df, freq=f'{w}D')\r\n",
    "            ax[i // 2, i % 2].plot(df_tmp['Value'], label=f'{w}-day {kind}')\r\n",
    "    ax[0, 0].legend()\r\n",
    "\r\n",
    "get_general_trends(df_dict=df_partitioned, ma_windows=[1, 7, 30], kind='ma')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_plots_by_gas(df_dict: pd.DataFrame):\r\n",
    "    def get_plot(df: pd.DataFrame, ax):\r\n",
    "        ax.plot(df['T'], label='T')\r\n",
    "        ax.plot(df['RH'], label='RH')\r\n",
    "        ax.plot([], [], color='g', label='Value')\r\n",
    "\r\n",
    "        ax2 = ax.twinx()\r\n",
    "        ax2.plot(df['Value'], color='green')\r\n",
    "        # ax.set_xticklabels(labels=df.index.month)\r\n",
    "\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\r\n",
    "    gg = [(0, 0), (0, 1), (1, 0), (1, 1)]\r\n",
    "    \r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        current_ax = ax[(i // 2, i % 2)]\r\n",
    "        df_tmp = df_dict[gas]\r\n",
    "        get_plot(df_tmp, ax=current_ax)\r\n",
    "        current_ax.set_title(gas)\r\n",
    "        cc = current_ax.get_xticks()\r\n",
    "    \r\n",
    "    ax[0, 0].legend()\r\n",
    "    fig.tight_layout()\r\n",
    "\r\n",
    "get_plots_by_gas(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_corr(df_dict: Dict[str, pd.DataFrame]):\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        current_ax = ax[i // 2, i % 2]\r\n",
    "        sns.heatmap(data=df_dict[gas].corr().abs(), annot=True, cmap='bone_r', vmin=0.0, vmax=1.0, ax=current_ax)\r\n",
    "        current_ax.set_title(gas)\r\n",
    "\r\n",
    "get_corr(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### After Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_partitioned = get_df_partitioned_by_gas(df=df, clean=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_exog_variable_distributions(df_dict=df_partitioned['CO'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_gas_value_distributions(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_general_trends(df_dict=df_partitioned, ma_windows=[1, 7, 30], kind='ma')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_plots_by_gas(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_corr(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Tools"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cf_summary(df: pd.DataFrame, kind: Union['pacf', 'acf'], threshold: float = 0.01, max_lag: int = 40, plot: bool = False):\r\n",
    "    from statsmodels.tsa.stattools import pacf, acf\r\n",
    "    from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\r\n",
    "    corr_fn = pacf if kind == 'pacf' else acf\r\n",
    "    plot_fn = plot_pacf if kind == 'pacf' else plot_acf\r\n",
    "    lag_space = min(max_lag, df.count() // 2 - 1)\r\n",
    "    corr_scores, conf_intvs = corr_fn(df, nlags=lag_space, alpha=threshold)\r\n",
    "    lower_conf_bound = conf_intvs[:, 0] - corr_scores\r\n",
    "    upper_conf_bound = conf_intvs[:, 1] - corr_scores\r\n",
    "    t_values = np.where((corr_scores < lower_conf_bound) | (corr_scores > upper_conf_bound))[0][1:]\r\n",
    "    best_t = t_values[np.argmax((np.abs(corr_scores[t_values])))] if len(t_values) > 0 else 0\r\n",
    "    if plot:\r\n",
    "        plot_fn(df, lags=lag_space, alpha=threshold, zero=False).get_axes()[0].set_title(f'possible t-values: {tuple(t_values)} Best t: {best_t}')\r\n",
    "    t_dict = {}\r\n",
    "    return t_values, best_t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_seasonal_periods(df_dict: Dict[str, pd.DataFrame], plot: bool = False):\r\n",
    "    value_dict = {}\r\n",
    "    if plot:\r\n",
    "        fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        df = df_dict[gas]\r\n",
    "        seasonal_component = seasonal_decompose(df['Value']).seasonal\r\n",
    "        _, m = cf_summary(seasonal_component, kind='acf')\r\n",
    "        value_dict[gas] = m\r\n",
    "        if plot:\r\n",
    "            focus = seasonal_component['2004-04-01':'2004-04-20']\r\n",
    "            current_ax = ax[i // 2, i % 2]\r\n",
    "            current_ax.plot(focus.index.day, focus)\r\n",
    "            current_ax.set_xticks(focus.index.day)\r\n",
    "            current_ax.set_title(gas)\r\n",
    "    return value_dict\r\n",
    "\r\n",
    "get_seasonal_periods(df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_stationarity(df: pd.DataFrame, cf_threshold: float, significance_level: float = 0.05, ax = None):\r\n",
    "    from statsmodels.tsa.statespace.tools import diff\r\n",
    "    p_value = adfuller(df['Value'])[1]\r\n",
    "    is_stationary = p_value < significance_level\r\n",
    "    p_values, best_p = cf_summary(diff(df['Value'], k_diff=0 if is_stationary else 1, k_seasonal_diff=1, seasonal_periods=7), kind='pacf', max_lag=6, threshold=cf_threshold)\r\n",
    "    q_values, best_q = cf_summary(diff(df['Value'], k_diff=0 if is_stationary else 1, k_seasonal_diff=1, seasonal_periods=7), kind='acf', max_lag=6, threshold=cf_threshold)\r\n",
    "    if ax is not None:\r\n",
    "        decomposition = seasonal_decompose(df['Value'], model='additive')\r\n",
    "        decomposition.plot(ax=ax)\r\n",
    "    return is_stationary, p_values, best_p, q_values, best_q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_param_summary(df_dict: Dict[str, pd.DataFrame], threshold: float):\r\n",
    "    summary_dict = {}\r\n",
    "    for gas in df_dict.keys():\r\n",
    "        stationary, p_values, best_p, q_values, best_q = test_stationarity(df_dict[gas], cf_threshold=threshold, significance_level=0.01)\r\n",
    "        summary_dict[gas] = {\r\n",
    "            'stationary': stationary,\r\n",
    "            'p_values': p_values,\r\n",
    "            'best_p': best_p,\r\n",
    "            'q_values': q_values,\r\n",
    "            'best_q': best_q\r\n",
    "        }\r\n",
    "    return summary_dict\r\n",
    "\r\n",
    "get_param_summary(df_partitioned, threshold=0.01)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# baseline prediction, since they are all stationary\r\n",
    "# test_set = pd.read_csv(filepath_or_buffer='./data/test.csv', sep=',')\r\n",
    "# pd.merge(left=test_set, right=df.groupby(by='Gas').mean(), left_on='Gas', right_index=True, how='inner')[['id', 'Value']].set_index(keys='id').to_csv('./out/using_mean.csv', sep=',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def convert_orders(l: List[int]):\r\n",
    "    list_tmp = [0 for _ in range(max(l))]\r\n",
    "    for i in l:\r\n",
    "        list_tmp[i - 1] = 1\r\n",
    "    return tuple(list_tmp)\r\n",
    "\r\n",
    "convert_orders([1, 6])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_params(df_dict: Dict[str, pd.DataFrame], threshold: float = 0.01):\r\n",
    "    param_summary = get_param_summary(df_dict, threshold=threshold)\r\n",
    "    orders = {}\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        p = convert_orders(param_summary[gas]['p_values'])\r\n",
    "        d = 0 if param_summary[gas]['stationary'] else 1\r\n",
    "        q = convert_orders(param_summary[gas]['q_values'])\r\n",
    "        orders[gas] = (p, d, q)\r\n",
    "    return orders\r\n",
    "\r\n",
    "get_params(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate_params(df: pd.DataFrame, algo: Union['sarima', 'sarimax', 'expsmooth'], init_kws: Dict, fit_kws: Dict, test_size: float = 0.75):\r\n",
    "    from sklearn.metrics import mean_squared_error\r\n",
    "    from statsmodels.tsa.holtwinters import ExponentialSmoothing\r\n",
    "    with warnings.catch_warnings():\r\n",
    "        warnings.simplefilter('ignore', ConvergenceWarning)\r\n",
    "        models = {}\r\n",
    "\r\n",
    "        partition_index = int(df.shape[0] * test_size)\r\n",
    "\r\n",
    "        train = df.iloc[:partition_index,:]\r\n",
    "        test = df.iloc[partition_index:,:]\r\n",
    "\r\n",
    "        if algo == 'sarima':\r\n",
    "            model = SARIMAX(endog=train['Value'], **init_kws).fit(**fit_kws)\r\n",
    "        elif algo == 'sarimax':\r\n",
    "            exog = init_kws.pop('exog')\r\n",
    "            model = SARIMAX(endog=train['Value'], exog=train[exog], **init_kws).fit(**fit_kws)\r\n",
    "        else:\r\n",
    "            model = ExponentialSmoothing(endog=train['Value'], **init_kws).fit(**fit_kws)\r\n",
    "\r\n",
    "        if algo == 'sarimax':\r\n",
    "            train_pred = model.predict(start=0, end=partition_index - 1, exog=train[exog])\r\n",
    "            test_pred = model.forecast(steps=df.shape[0] - partition_index, exog=test[exog])\r\n",
    "        else:\r\n",
    "            train_pred = model.predict(start=0, end=partition_index - 1)\r\n",
    "            test_pred = model.forecast(steps=df.shape[0] - partition_index)\r\n",
    "        # test_pred = model.predict(start=partition_index, end=data.shape[0] - 1)\r\n",
    "\r\n",
    "        train_err = mean_squared_error(train['Value'], train_pred, squared=False)\r\n",
    "        test_err = mean_squared_error(test['Value'], test_pred, squared=False)\r\n",
    "\r\n",
    "        return {\r\n",
    "            'aic': model.aic,\r\n",
    "            'train_true': train,\r\n",
    "            'train_pred': train_pred,\r\n",
    "            'train_rmse': train_err,\r\n",
    "            'test_true': test,\r\n",
    "            'test_pred': test_pred,\r\n",
    "            'test_rmse': test_err\r\n",
    "        }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def grid_search_sarima(df_dict: Dict[str, pd.DataFrame]):\r\n",
    "    from numpy.linalg import LinAlgError\r\n",
    "    gs_summary = {\r\n",
    "        'CO': [],\r\n",
    "        'NOx': [],\r\n",
    "        'NMHC': [],\r\n",
    "        'O3': []\r\n",
    "    }\r\n",
    "    seasonal_params = {\r\n",
    "        'P_space': [0, 1, 2],\r\n",
    "        'D_space': [0, 1],\r\n",
    "        'Q_space': [0, 1, 2]\r\n",
    "    }\r\n",
    "    seasonal_periods = get_seasonal_periods(df_dict=df_dict)\r\n",
    "    trend_orders = get_params(df_dict=df_dict, threshold=0.01)\r\n",
    "    for gas in df_dict.keys():\r\n",
    "        T = trend_orders[gas]\r\n",
    "        m = seasonal_periods[gas]\r\n",
    "        df = df_dict[gas]\r\n",
    "        for trend in ['n', 'c']:\r\n",
    "            for P in seasonal_params['P_space']:\r\n",
    "                for D in seasonal_params['D_space']:\r\n",
    "                    for Q in seasonal_params['Q_space']:\r\n",
    "                        try:\r\n",
    "                            results = evaluate_params(df, algo='sarima', init_kws={'order': T, 'seasonal_order': (P, D, Q, m), 'trend': trend}, fit_kws={})\r\n",
    "                            gs_summary[gas].append({\r\n",
    "                                'params': {\r\n",
    "                                    'init': {\r\n",
    "                                        'order': T,\r\n",
    "                                        'seasonal_order': (P, D, Q, m),\r\n",
    "                                        'trend': trend\r\n",
    "                                    },\r\n",
    "                                    'fit': {}\r\n",
    "                                },\r\n",
    "                                'aic': results['aic'],\r\n",
    "                                'train_rmse': results['train_rmse'],\r\n",
    "                                'test_rmse': results['test_rmse']\r\n",
    "                            })\r\n",
    "                        except LinAlgError, ValueError:\r\n",
    "                            continue\r\n",
    "    return gs_summary\r\n",
    "\r\n",
    "import pickle\r\n",
    "gs_sarima = grid_search_sarima(df_partitioned)\r\n",
    "pickle.dump(obj=gs_sarima, file=open('./tmp/models/gs-sarima-drop-outliers.p', 'wb'))\r\n",
    "# gs_sarima = pickle.load(file=open('./tmp/models/gs-sarima-drop-outliers.p', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def grid_search_exp_smth(df_dict: Dict[str, pd.DataFrame]):\r\n",
    "    from numpy.linalg import LinAlgError\r\n",
    "    gs_summary = {\r\n",
    "        'CO': [],\r\n",
    "        'NOx': [],\r\n",
    "        'NMHC': [],\r\n",
    "        'O3': []\r\n",
    "    }\r\n",
    "    trend_types = ['add', 'mul']\r\n",
    "    seasonal_types = ['add', 'mul']\r\n",
    "    seasonal_periods = get_seasonal_periods(df_dict=df_dict)\r\n",
    "    smoothing_levels = np.linspace(0.01, 0.9, 8)\r\n",
    "    for gas in df_dict.keys():\r\n",
    "        for smoothing_level in smoothing_levels:\r\n",
    "            for t in trend_types:\r\n",
    "                for s in seasonal_types:\r\n",
    "                    df = df_dict[gas]\r\n",
    "                    try:\r\n",
    "                        results = evaluate_params(df, algo='expsmooth', init_kws={'trend': t, 'seasonal': s, 'seasonal_periods': seasonal_periods[gas]}, fit_kws={'smoothing_level': smoothing_level})\r\n",
    "                        gs_summary[gas].append({\r\n",
    "                            'params': {\r\n",
    "                                'init': {\r\n",
    "                                    'trend': t,\r\n",
    "                                    'seasonal': s\r\n",
    "                                },\r\n",
    "                                'fit': {\r\n",
    "                                    'smoothing_level': smoothing_level\r\n",
    "                                }\r\n",
    "                            },\r\n",
    "                            'aic': results['aic'],\r\n",
    "                            'train_rmse': results['train_rmse'],\r\n",
    "                            'test_rmse': results['test_rmse']\r\n",
    "                        })\r\n",
    "                    except LinAlgError:\r\n",
    "                        continue\r\n",
    "    return gs_summary\r\n",
    "\r\n",
    "# gs_expsmth = grid_search_exp_smth(df_partitioned)\r\n",
    "gs_expsmth = pickle.load(file=open('./tmp/models/gs-expsmth-drop-outliers.p', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate_grid_search(gs_results, metric: Union['rmse', 'aic'] = 'rmse'):\r\n",
    "    gs_best_params = {}\r\n",
    "    for gas in gs_results.keys():\r\n",
    "        res = gs_results[gas]\r\n",
    "        index = np.argmin(list(map(lambda x: x['test_rmse'], res))) if metric == 'rmse' else np.argmin(list(map(lambda x: x['aic'], res)))\r\n",
    "        print(gas)\r\n",
    "        print(res[index])\r\n",
    "        best_params = res[index]['params']\r\n",
    "        gs_best_params[gas] = {\r\n",
    "            'params': best_params,\r\n",
    "            'rmse': res[index]['test_rmse']\r\n",
    "        }\r\n",
    "    return gs_best_params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sarima_best = evaluate_grid_search(gs_sarima, metric='rmse')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "exp_smth_best = evaluate_grid_search(gs_expsmth, metric='rmse')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def grid_search_sarimax(df_dict: Dict[str, pd.DataFrame], sarima_best):\r\n",
    "    gs_summary = {\r\n",
    "        'CO': [],\r\n",
    "        'NOx': [],\r\n",
    "        'NMHC': [],\r\n",
    "        'O3': []\r\n",
    "    }\r\n",
    "    from numpy.linalg import LinAlgError\r\n",
    "    for gas in df_dict.keys():\r\n",
    "        df = df_dict[gas]\r\n",
    "        current_sarima_best = sarima_best[gas]\r\n",
    "        for exog in ['T', 'RH', ['T', 'RH']]:\r\n",
    "            try:\r\n",
    "                results = evaluate_params(df, algo='sarimax', init_kws={\r\n",
    "                    **current_sarima_best['params']['init'],\r\n",
    "                    'exog': exog\r\n",
    "                }, fit_kws={})\r\n",
    "                gs_summary[gas].append({\r\n",
    "                    'params': {\r\n",
    "                        'init': {\r\n",
    "                            **current_sarima_best['params']['init'],\r\n",
    "                            'exog': exog\r\n",
    "                        },\r\n",
    "                        'fit': {}\r\n",
    "                    },\r\n",
    "                    'aic': results['aic'],\r\n",
    "                    'train_rmse': results['train_rmse'],\r\n",
    "                    'test_rmse': results['test_rmse']\r\n",
    "                })\r\n",
    "            except LinAlgError:\r\n",
    "                continue\r\n",
    "    return gs_summary\r\n",
    "\r\n",
    "gs_sarimax = grid_search_sarimax(df_partitioned, sarima_best)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sarimax_best = evaluate_grid_search(gs_sarimax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compare_algorithms_for_each_gas(sarima_gs: Dict[str, Dict], sarimax_gs: Dict[str, Dict], expsmth_gs: Dict[str, Dict]):\r\n",
    "    best_algorithms = {} \r\n",
    "    for gas in sarima_gs.keys():\r\n",
    "        current_best_sarima = sarima_gs[gas]\r\n",
    "        current_best_sarimax = sarimax_gs[gas]\r\n",
    "        current_best_expsmth = expsmth_gs[gas]\r\n",
    "\r\n",
    "        sarima_rmse = current_best_sarima['rmse']\r\n",
    "        sarimax_rmse = current_best_sarimax['rmse']\r\n",
    "        expsmth_rmse = current_best_expsmth['rmse']\r\n",
    "\r\n",
    "        best_model = ['sarima', 'sarimax', 'expsmth'][np.argmin([sarima_rmse, sarimax_rmse, expsmth_rmse])]\r\n",
    "\r\n",
    "        if best_model == 'sarima':\r\n",
    "            model = SARIMAX\r\n",
    "            init = current_best_sarima['params']['init']\r\n",
    "            fit = current_best_sarima['params']['fit']\r\n",
    "        elif best_model == 'sarimax':\r\n",
    "            model = SARIMAX\r\n",
    "            init = current_best_sarimax['params']['init']\r\n",
    "            fit = current_best_sarimax['params']['fit']\r\n",
    "        else:\r\n",
    "            model = ExponentialSmoothing\r\n",
    "            init = current_best_expsmth['params']['init']\r\n",
    "            fit = current_best_expsmth['params']['fit']\r\n",
    "\r\n",
    "        best_algorithms[gas] = {\r\n",
    "            'model': model[0] if type(model) is tuple else model,\r\n",
    "            'init': init,\r\n",
    "            'fit': fit\r\n",
    "        }\r\n",
    "    return best_algorithms\r\n",
    "\r\n",
    "algo_comparison = compare_algorithms_for_each_gas(sarima_best, sarimax_best, exp_smth_best)\r\n",
    "algo_comparison"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "def evaluate_params_whole(df_dict: Dict[str, pd.DataFrame], models: Dict[str, Dict], test_size: float = 0.75):\r\n",
    "    with warnings.catch_warnings():\r\n",
    "        warnings.simplefilter('ignore', ConvergenceWarning)\r\n",
    "        results = {}\r\n",
    "        for gas in df_dict.keys():\r\n",
    "            current_config = models[gas]\r\n",
    "            algo = current_config['model']\r\n",
    "            init_params = current_config['init']\r\n",
    "            fit_params = current_config['fit']\r\n",
    "\r\n",
    "            df = df_dict[gas]\r\n",
    "            partition_index = int(df.shape[0] * test_size)\r\n",
    "\r\n",
    "            train = df[:partition_index]\r\n",
    "            test = df[partition_index:]\r\n",
    "\r\n",
    "            if 'exog' in init_params:\r\n",
    "                exog = init_params.pop('exog')\r\n",
    "                model = algo(endog=train['Value'], exog=train[exog], **init_params).fit(**fit_params)\r\n",
    "                train_pred = model.predict(start=0, end=partition_index - 1, exog=train[exog])\r\n",
    "                test_pred = model.forecast(steps=df.shape[0] - partition_index, exog=test[exog])\r\n",
    "            else:\r\n",
    "                model = algo(endog=train['Value'], **init_params).fit(**fit_params)\r\n",
    "                train_pred = model.predict(start=0, end=partition_index - 1)\r\n",
    "                test_pred = model.forecast(steps=df.shape[0] - partition_index)\r\n",
    "            # test_pred = model.predict(start=partition_index, end=df.shape[0] - 1)\r\n",
    "\r\n",
    "            train_err = mean_squared_error(train[7:]['Value'], train_pred[7:], squared=False)\r\n",
    "            test_err = mean_squared_error(test['Value'], test_pred, squared=False)\r\n",
    "\r\n",
    "            results[gas] = {\r\n",
    "                'aic': model.aic,\r\n",
    "                'train_true': train['Value'],\r\n",
    "                'train_pred': train_pred,\r\n",
    "                'train_rmse': train_err,\r\n",
    "                'test_true': test['Value'],\r\n",
    "                'test_pred': test_pred,\r\n",
    "                'test_rmse': test_err\r\n",
    "            }\r\n",
    "        return results\r\n",
    "\r\n",
    "gg = evaluate_params_whole(df_partitioned, models=algo_comparison)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def decompose(results: Dict[str, Dict], test_rmse_weight: float = 0.5):\r\n",
    "    aic_values = []\r\n",
    "    train_rmse_values = []\r\n",
    "    test_rmse_values = []\r\n",
    "    for gas in results.keys():\r\n",
    "        aic_values.append(results[gas]['aic'])\r\n",
    "        train_rmse_values.append(results[gas]['train_rmse'])\r\n",
    "        test_rmse_values.append(results[gas]['test_rmse'])\r\n",
    "    decomposed_results = {\r\n",
    "        'mean_aic': sum(aic_values) / len(aic_values),\r\n",
    "        'mean_train_rmse': sum(train_rmse_values) / len(train_rmse_values),\r\n",
    "        'mean_test_rmse': sum(test_rmse_values) / len(test_rmse_values),\r\n",
    "    }\r\n",
    "    decomposed_results['overall_mean_rmse'] = (1 - test_rmse_weight) * decomposed_results['mean_train_rmse'] + test_rmse_weight * decomposed_results['mean_test_rmse']\r\n",
    "    return decomposed_results\r\n",
    "\r\n",
    "decompose(gg, test_rmse_weight=0.75)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_predictions(results: Dict[str, Dict]):\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 7))\r\n",
    "    for i, gas in enumerate(results.keys()):\r\n",
    "        current_results = results[gas]\r\n",
    "        current_ax = ax[i // 2, i % 2]\r\n",
    "        current_ax.plot(pd.concat(objs=(current_results['train_true'], current_results['test_true']), axis=0))\r\n",
    "        current_ax.plot(current_results['train_pred'])\r\n",
    "        current_ax.plot(current_results['test_pred'])\r\n",
    "        current_ax.set_title(gas)\r\n",
    "\r\n",
    "plot_predictions(gg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_test_df(file_path: str = './data/test.csv'):\r\n",
    "    test_df = pd.read_csv(filepath_or_buffer=file_path, sep=',', header=0)\r\n",
    "    test_df['Date'] = pd.to_datetime(test_df['Date'], format='%d/%m/%Y')\r\n",
    "    test_df.set_index(keys='Date', inplace=True)\r\n",
    "    return test_df.sort_index(ascending=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def kaggle_submission(df_dict: Dict[str, pd.DataFrame], params: Dict[str, Dict], test_file: str, submission_file: str, training_size: float = 1.0, recent: bool = True):\r\n",
    "    test_df = get_test_df()\r\n",
    "    result_set = {}\r\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\r\n",
    "    from numpy.linalg import LinAlgError\r\n",
    "    with warnings.catch_warnings():\r\n",
    "        warnings.simplefilter('ignore', ConvergenceWarning)\r\n",
    "        for gas in df_dict.keys():\r\n",
    "            df = df_dict[gas]\r\n",
    "            df = df.iloc[int((1 - training_size) * df.shape[0]):] if recent else df.iloc[:int(training_size * df.shape[0])]\r\n",
    "\r\n",
    "            if gas == 'NMHC':\r\n",
    "                df = df.loc[:'2004-10-01']\r\n",
    "            target = test_df[test_df['Gas'] == gas]\r\n",
    "\r\n",
    "            current_config = params[gas]\r\n",
    "            algo = current_config['model']\r\n",
    "            init_params = current_config['init']\r\n",
    "            fit_params = current_config['fit']\r\n",
    "\r\n",
    "            if 'exog' in init_params.keys():\r\n",
    "                exog = init_params.pop('exog')\r\n",
    "                model = algo(endog=df['Value'], exog=df[exog], **init_params).fit(**fit_params)\r\n",
    "                pred = model.forecast(\r\n",
    "                    steps=len(pd.date_range(df.index[-1], target.index[-1],\r\n",
    "                    freq='D',\r\n",
    "                    exog=df[exog]))).rename('Value')\r\n",
    "            else:\r\n",
    "                model = algo(endog=df['Value'], **init_params).fit(**fit_params)\r\n",
    "                pred = model.forecast(\r\n",
    "                    steps=len(pd.date_range(df.index[-1], target.index[-1],\r\n",
    "                    freq='D'))).rename('Value')\r\n",
    "\r\n",
    "            result = pd.merge(left=pred, right=target, left_index=True, right_index=True, how='inner')\r\n",
    "            print(result.shape)\r\n",
    "            result_set[gas] = result[['id', 'T', 'RH', 'Value']].set_index(keys='id')\r\n",
    "\r\n",
    "    pred_df = pd.DataFrame(columns=result_set['CO'].columns)\r\n",
    "    for gas in result_set.keys():\r\n",
    "        pred_df = pd.concat(objs=(pred_df, result_set[gas]), axis=0)\r\n",
    "    \r\n",
    "    pred_df.sort_index(inplace=True)\r\n",
    "    pred_df['Value'].to_csv(submission_file, sep=',', index_label='id')\r\n",
    "    \r\n",
    "    return pred_df.sort_index(ascending=True)\r\n",
    "\r\n",
    "pred_df = kaggle_submission(df_partitioned, training_size=1.0, params=algo_comparison, test_file='./data/test.csv', submission_file='./out/sarima-sarimax-expsmth-exog-earlier.csv', recent=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_forecasts(df_dict: Dict[str, pd.DataFrame], submission_file: str):\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\r\n",
    "    test_df = get_test_df()\r\n",
    "    pred_df = pd.read_csv(submission_file, index_col=0)\r\n",
    "    for i, gas in enumerate(test_df['Gas'].unique()):\r\n",
    "        df = df_dict[gas]\r\n",
    "        current_ax = ax[i // 2, i % 2]\r\n",
    "        target = test_df[test_df['Gas'] == gas]\r\n",
    "        future = pd.merge(left=target, right=pred_df, left_on='id', right_index=True, how='inner')\r\n",
    "        current_ax.plot(df['Value'])\r\n",
    "        current_ax.plot(future['Value'])\r\n",
    "        current_ax.set_title(gas)\r\n",
    "\r\n",
    "plot_forecasts(df_partitioned, submission_file='./out/sarima-sarimax-expsmth-exog-earlier.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def bar_forecasts():\r\n",
    "    df = pd.read_csv('./out/sarima-sarimax-expsmth-exog.csv', index_col=0)\r\n",
    "    df.plot()\r\n",
    "\r\n",
    "bar_forecasts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm Selection and Hyper-Parameter Tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part B > Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Exclusive Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation, AgglomerativeClustering"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2 = pd.read_csv('./data/Mall_Customers.csv', index_col=0)\r\n",
    "df2.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rename_columns(df: pd.DataFrame):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    df_tmp.rename(mapper={\r\n",
    "        'Genre': 'Gender',\r\n",
    "        'Annual Income (k$)': 'Annual Income',\r\n",
    "        'Spending Score (1-100)': 'Spending Score'\r\n",
    "    }, axis=1, inplace=True)\r\n",
    "    # df_tmp['Annual Income'] = df_tmp['Annual Income'] * 1000\r\n",
    "    return df_tmp\r\n",
    "\r\n",
    "df2 = rename_columns(df2)\r\n",
    "df2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_distribution_of_each_variable(df: pd.DataFrame):\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\r\n",
    "    for i, c in enumerate(df.columns):\r\n",
    "        var = df[c]\r\n",
    "        kde = var.dtype.kind in 'biufc'\r\n",
    "        sns.histplot(var, kde=kde, ax=ax[i // 2, i % 2])\r\n",
    "\r\n",
    "get_distribution_of_each_variable(df2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_distribution_by_gender(df: pd.DataFrame):\r\n",
    "    fig, ax = plt.subplots(nrows=3, figsize=(8, 8))\r\n",
    "    for i, c in enumerate(['Age', 'Annual Income', 'Spending Score']):\r\n",
    "        var = df[c]\r\n",
    "        sns.kdeplot(x=var, hue=df['Gender'], ax=ax[i])\r\n",
    "    fig.tight_layout()\r\n",
    "\r\n",
    "get_distribution_by_gender(df2)\r\n",
    "\r\n",
    "# It seems that there are slightly more data on female customers than male ones"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_corr_heatmap(df: pd.DataFrame):\r\n",
    "    sns.heatmap(df.corr(), annot=True, cmap='bone')\r\n",
    "\r\n",
    "get_corr_heatmap(df2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def standardize_variables(df: pd.DataFrame, cols: List[str]):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    df_tmp[cols] = StandardScaler().fit_transform(df_tmp[cols])\r\n",
    "    return df_tmp\r\n",
    "\r\n",
    "df2_s = standardize_variables(df2, cols=['Age', 'Annual Income', 'Spending Score'])\r\n",
    "df2_s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Principal Component Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from typing import Union, List\r\n",
    "\r\n",
    "def get_pca_results(df: pd.DataFrame, ignore_cols: Union[str, List[str]]):\r\n",
    "    df_scaled = StandardScaler().fit_transform(X=df.drop(columns=ignore_cols))\r\n",
    "\r\n",
    "    pca = PCA(n_components=df_scaled.shape[1]).fit(X=df_scaled)\r\n",
    "    header = ['Eigenvalue', 'Explained Variance', 'Cumulative Explained Variance']\r\n",
    "    header.extend(df.drop(columns=ignore_cols).columns.tolist())\r\n",
    "    eigenvalues = pca.explained_variance_\r\n",
    "    eigenvectors = pca.components_\r\n",
    "    expl_var = pca.explained_variance_ratio_\r\n",
    "    cum_expl_var = pca.explained_variance_ratio_.cumsum()\r\n",
    "    pca_results = pd.DataFrame(\r\n",
    "        data=np.hstack((\r\n",
    "            eigenvalues.reshape(-1, 1),\r\n",
    "            expl_var.reshape(-1, 1),\r\n",
    "            cum_expl_var.reshape(-1, 1),\r\n",
    "            eigenvectors\r\n",
    "        )),\r\n",
    "        columns=header,\r\n",
    "        index=[f'PC {i + 1}' for i in range(df_scaled.shape[1])]\r\n",
    "    )\r\n",
    "\r\n",
    "    df_transformed = pd.DataFrame(\r\n",
    "        data=pca.transform(df_scaled),\r\n",
    "        index=df.index,\r\n",
    "        columns=[f'PC {i + 1}' for i in range(df_scaled.shape[1])]\r\n",
    "    )\r\n",
    "\r\n",
    "    return pca_results, df_transformed\r\n",
    "\r\n",
    "pca_results, df2_transformed = get_pca_results(df=df2, ignore_cols='Gender')\r\n",
    "pca_results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scree_plot(df: pd.DataFrame, pca: pd.DataFrame):\r\n",
    "    with sns.axes_style(style='darkgrid'):\r\n",
    "        ax = sns.pointplot(data=pca, x=pca.index, y=pca['Eigenvalue'])\r\n",
    "        ax.set(\r\n",
    "            title='Scree Plot for PCA (df2)',\r\n",
    "            ylim=(0, 1.4)\r\n",
    "        )\r\n",
    "        ax.annotate(text='As there is no elbow,\\nno PC should be discarded', xy=(1.75, 1.2), ha='center')\r\n",
    "        return ax\r\n",
    "\r\n",
    "scree_plot(df2, pca_results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visual Analysis - Compare Algorithms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import plotly.express as px\r\n",
    "from sklearn.cluster import DBSCAN, OPTICS, AgglomerativeClustering, AffinityPropagation\r\n",
    "\r\n",
    "for mo in [KMeans(n_clusters=5), DBSCAN(eps=15, min_samples=15), OPTICS(max_eps=18), AgglomerativeClustering(n_clusters=6), AffinityPropagation()]:\r\n",
    "    data = df2[['Annual Income', 'Age', 'Spending Score']]\r\n",
    "    colrs = mo.fit_predict(X=data)\r\n",
    "    fig = px.scatter_3d(data_frame=df2, x='Annual Income', y='Age', z='Spending Score', color=colrs, title=type(mo).__name__ + ' ' + str(round(silhouette_score(X=data, labels=colrs), 2)), color_continuous_scale=px.colors.sequential.Rainbow)\r\n",
    "    fig.show()\r\n",
    "\r\n",
    "# both kmeans and agglomerative clustering show promise"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Numerical Optimisation - Hyperparameter Selection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_dist_score(df: pd.DataFrame, cols: List[str], clusters: int, plot: bool = False):\r\n",
    "    model = KMeans(n_clusters=clusters, random_state=3).fit(X=df[cols])\r\n",
    "    centers = model.cluster_centers_\r\n",
    "    y_hat = model.predict(X=df[cols])\r\n",
    "    \r\n",
    "    df_tmp = pd.concat(objs=(df, pd.Series(data=y_hat, name='Class', index=df.index)), axis=1)\r\n",
    "    means = []\r\n",
    "    counts = []\r\n",
    "    for i, c in enumerate(centers):\r\n",
    "        class_i = df_tmp[df_tmp['Class'] == i][cols]\r\n",
    "        distances = np.linalg.norm(class_i - centers[i, :], axis=1)\r\n",
    "        means.append(distances.mean())\r\n",
    "        counts.append(len(class_i))\r\n",
    "    \r\n",
    "    if plot == True:\r\n",
    "        for i, m in enumerate(means):\r\n",
    "            plt.plot(centers[i,0], centers[i,1], 'o', mfc='none', color='r', markersize=m * 5)\r\n",
    "        sns.scatterplot(x=df['Annual Income'], y=df['Spending Score'], hue=y_hat)\r\n",
    "\r\n",
    "    return np.std(means) / (sum(means) / len(means)), np.std(counts)\r\n",
    "    # return np.std(means)\r\n",
    "\r\n",
    "get_dist_score(df2, ['Age', 'Annual Income', 'Spending Score'], 6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_silhouette_score(df: pd.DataFrame, cols: List[str], clusters: int):\r\n",
    "    from sklearn.metrics import silhouette_score\r\n",
    "    X = df[cols]\r\n",
    "    y_hat = KMeans(n_clusters=clusters).fit(X=X).predict(X=X)\r\n",
    "    return silhouette_score(X=df[cols], labels=y_hat)\r\n",
    "\r\n",
    "get_silhouette_score(df=df2, cols=['Age', 'Annual Income', 'Spending Score'], clusters=6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# type: ignore\r\n",
    "from sklearn.metrics import silhouette_score\r\n",
    "from more_itertools import powerset\r\n",
    "\r\n",
    "def get_custom_score_plot(df: pd.DataFrame, cols: List[str], clusters: List[int] = list(range(2, 11)), ignore_cols: Union[str, List[str]] = None):\r\n",
    "    distance_scores = [get_dist_score(df, cols, n)[0] for n in clusters]\r\n",
    "    count_scores = [get_dist_score(df, cols, n)[1] for n in clusters]\r\n",
    "    silhouette_scores = [get_silhouette_score(df, cols, n) for n in clusters]\r\n",
    "\r\n",
    "    ax = sns.lineplot(x=clusters, y=silhouette_scores)\r\n",
    "    ax2 = ax.twinx()\r\n",
    "    ax3 = ax.twinx()\r\n",
    "    sns.lineplot(x=clusters, y=distance_scores, color='g', ax=ax2)\r\n",
    "    sns.lineplot(x=clusters, y=count_scores, color='r', ax=ax3)\r\n",
    "\r\n",
    "get_custom_score_plot(df=df2, cols=['Age', 'Annual Income', 'Spending Score'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_custom_score_plot(df=df2, cols=['Annual Income', 'Spending Score'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import plotly.express as px\r\n",
    "\r\n",
    "def get_final_plot(df: pd.DataFrame):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    X = df[['Age', 'Annual Income', 'Spending Score']]\r\n",
    "    model = KMeans(n_clusters=6)\r\n",
    "    model.fit(X=X)\r\n",
    "    y_hat = model.predict(X=X)\r\n",
    "    df_tmp['Class'] = y_hat\r\n",
    "    fig = px.scatter_3d(data_frame=df_tmp, x='Age', y='Annual Income', z='Spending Score', color='Class')\r\n",
    "    return fig\r\n",
    "\r\n",
    "get_final_plot(df2)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}