{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit ('.venv': pipenv)"
  },
  "interpreter": {
   "hash": "b85dc5435e0bfe71fff45c7bd600e0d35505844357c06700d87c8785fb37e4d4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AIML CA2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import General Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Data Manipulation Dependencies\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Graphing Dependencies\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import plotly.express as px\r\n",
    "\r\n",
    "# Miscellaneous Dependencies\r\n",
    "from typing import Union, List, Tuple, Dict, Callable\r\n",
    "import pickle\r\n",
    "import warnings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Hide all warnings\r\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\r\n",
    "from numpy.linalg import LinAlgError\r\n",
    "\r\n",
    "warnings.filterwarnings(action='ignore')\r\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part A > Time Series Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Exclusive Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Time Series Tools\r\n",
    "from statsmodels.tsa.statespace.tools import diff\r\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\r\n",
    "\r\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\r\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\r\n",
    "\r\n",
    "# Time Series Models\r\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\r\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\r\n",
    "\r\n",
    "# For data cleaning\r\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv(filepath_or_buffer='./data/train.csv', sep=',', header=0)\r\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\r\n",
    "df.set_index(keys='Date', inplace=True)\r\n",
    "df.sort_index(inplace=True)\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_for_missing_values(df: pd.DataFrame):\r\n",
    "    df_tmp = df.groupby(by=[df.index.year, df.index.quarter]).count()\r\n",
    "    df_tmp.index.set_names(names=['Year', 'Quarter'], inplace=True)\r\n",
    "    print(df_tmp, end='\\n\\n')\r\n",
    "\r\n",
    "    complete_date_range = pd.date_range(start=df.index[0], end=df.index[-1], freq='D')\r\n",
    "    print(f'Number of missing rows: {4 * len(complete_date_range) - df.shape[0]}')\r\n",
    "\r\n",
    "check_for_missing_values(df=df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def groupby_date(df: pd.DataFrame, freq: str = 'D'):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    return df_tmp.resample(rule=freq).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def drop_outliers(df: pd.DataFrame, gas: str, columns: List[str], band: float = 1.5):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    outliers = {}\r\n",
    "    for c in columns:\r\n",
    "        vals = df[c].values\r\n",
    "        Q1 = np.quantile(vals, q=0.25)\r\n",
    "        Q3 = np.quantile(vals, q=0.75)\r\n",
    "        IQR = Q3 - Q1\r\n",
    "\r\n",
    "        lower_bound = Q1 - band * IQR\r\n",
    "        upper_bound = Q3 + band * IQR\r\n",
    "\r\n",
    "        df_tmp[c][(df[c] < lower_bound) | (df[c] > upper_bound)] = np.nan\r\n",
    "        print(f'Dropped {df_tmp[c][(df[c] < lower_bound) | (df[c] > upper_bound)].shape[0]} values for {gas}: {c} (outliers)')\r\n",
    "\r\n",
    "        outliers[c] = {\r\n",
    "            'upper_bound': upper_bound,\r\n",
    "            'lower_bound': lower_bound\r\n",
    "        }\r\n",
    "    return df_tmp, outliers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DataCleaner(BaseEstimator, TransformerMixin):\r\n",
    "    def __init__(self, boundaries):\r\n",
    "        self.boundaries = boundaries\r\n",
    "    \r\n",
    "    def fit(self, X: pd.DataFrame):\r\n",
    "        return self\r\n",
    "\r\n",
    "    def transform(self, X: pd.DataFrame):\r\n",
    "        X_tmp = X.copy()\r\n",
    "        for col in ['T', 'RH']:\r\n",
    "            current_col = X_tmp[col]\r\n",
    "            X_tmp[col][(current_col < self.boundaries[col]['lower_bound']) | (current_col > self.boundaries[col]['upper_bound'])] = np.nan\r\n",
    "        return X_tmp.interpolate(method='time')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def impute_missing_values(df: pd.DataFrame, gas: str, columns: List[str], method: str = 'time'):\r\n",
    "    missing_values_count = df.isna().sum()\r\n",
    "    for c in columns:\r\n",
    "        print(f'Inserted {missing_values_count[c]} values for {gas}: {c} (missing)')\r\n",
    "    return df.interpolate(method=method)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_df_partitioned_by_gas(df: pd.DataFrame, clean_columns: List[str] = None):\r\n",
    "    gas_dict = {}\r\n",
    "    gas_boundaries = {}\r\n",
    "    for gas in df['Gas'].unique():\r\n",
    "        df_tmp = df[df['Gas'] == gas]\r\n",
    "        if clean_columns is not None:\r\n",
    "            df_tmp, outliers = drop_outliers(df_tmp, gas, columns=clean_columns)\r\n",
    "            df_tmp = impute_missing_values(df_tmp, gas, columns=clean_columns)\r\n",
    "            gas_boundaries[gas] = outliers\r\n",
    "        df_tmp['Gas'] = [gas] * df_tmp.shape[0]\r\n",
    "        gas_dict[gas] = df_tmp\r\n",
    "    if clean_columns is not None:\r\n",
    "        return gas_dict, gas_boundaries\r\n",
    "    return gas_dict\r\n",
    "\r\n",
    "df_partitioned = get_df_partitioned_by_gas(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Before Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_gas_distributions(df: pd.DataFrame):\r\n",
    "    counts = df.groupby(by='Gas').count().min(axis=1).astype(int)\r\n",
    "    ax = sns.barplot(x=counts.index, y=counts, palette='deep')\r\n",
    "    ax.set_title('Counts of each gas')\r\n",
    "\r\n",
    "get_gas_distributions(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_gas_means_and_medians(df):\r\n",
    "    df_tmp = df.groupby(by='Gas', as_index=False)\r\n",
    "    aggs = ['Mean', 'Median']\r\n",
    "    comb_df = pd.DataFrame()\r\n",
    "    for i, frame in enumerate([df_tmp.mean(), df_tmp.median()]):\r\n",
    "        tmp_df = frame\r\n",
    "        tmp_df['Type'] = aggs[i]\r\n",
    "        comb_df = pd.concat(objs=(comb_df, tmp_df), axis=0)\r\n",
    "    ax = sns.barplot(data=comb_df, x='Gas', y='Value', hue='Type', palette='rainbow')\r\n",
    "    ax.set_title('Mean and Median of each gas')\r\n",
    "\r\n",
    "get_gas_means_and_medians(df)\r\n",
    "\r\n",
    "# The values for the gases seem positively skewed as mean < median"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_exog_variable_distributions(df: pd.DataFrame):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\r\n",
    "    ax1 = sns.histplot(x=df_tmp['T'], kde=True, ax=ax[0])\r\n",
    "    ax1.set_title('Temperature Distribution')\r\n",
    "    ax2 = sns.histplot(x=df_tmp['RH'], kde=True, ax=ax[1])\r\n",
    "    ax2.set_title('Relative Humidity Distribution')\r\n",
    "\r\n",
    "get_exog_variable_distributions(df_partitioned['CO'])\r\n",
    "\r\n",
    "# Temperature and Relative Humidity are positively skewed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_gas_value_distributions(df_dict: Dict[str, pd.DataFrame]):\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 6))\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        current_ax = ax[i // 2, i % 2]\r\n",
    "        current_ax.set_title(gas)\r\n",
    "        sns.histplot(data=df_dict[gas]['Value'], kde=True, ax=current_ax)\r\n",
    "    fig.tight_layout()\r\n",
    "\r\n",
    "get_gas_value_distributions(df_partitioned)\r\n",
    "\r\n",
    "# The values for all the gases are positively skewed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_general_trends(df_dict: Dict[str, pd.DataFrame], ma_windows: List[int] = [1, 7, 30], kind: str = 'group'):\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 7))\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        df = df_dict[gas]\r\n",
    "        current_ax = ax[i //2, i % 2]\r\n",
    "        for w in ma_windows:\r\n",
    "            df_tmp = df.rolling(window=w).mean() if kind == 'ma' else groupby_date(df, freq=f'{w}D')\r\n",
    "            current_ax.plot(df_tmp['Value'], label=f'{w}-day {kind}')\r\n",
    "        current_ax.set_title(gas)\r\n",
    "        default_xticks = current_ax.get_xticks()\r\n",
    "        current_ax.set_xticks([default_xticks[j] for j in range(len(default_xticks)) if j % 3 == 0])\r\n",
    "    ax[0, 0].legend()\r\n",
    "\r\n",
    "get_general_trends(df_dict=df_partitioned, ma_windows=[1, 7, 30], kind='ma')\r\n",
    "\r\n",
    "# There are a lot of outliers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_plots_by_gas(df_dict: pd.DataFrame):\r\n",
    "    def get_plot(df: pd.DataFrame, ax):\r\n",
    "        ax.plot(df['T'], label='T')\r\n",
    "        ax.plot(df['RH'], label='RH')\r\n",
    "        ax.plot([], [], color='g', label='Value')\r\n",
    "\r\n",
    "        ax2 = ax.twinx()\r\n",
    "        ax2.plot(df['Value'], color='green')\r\n",
    "        default_xticks = ax.get_xticks()\r\n",
    "        ax.set_xticks([default_xticks[j] for j in range(len(default_xticks)) if j % 3 == 0])\r\n",
    "\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 7))\r\n",
    "    \r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        current_ax = ax[(i // 2, i % 2)]\r\n",
    "        df_tmp = df_dict[gas]\r\n",
    "        get_plot(df_tmp, ax=current_ax)\r\n",
    "        current_ax.set_title(gas)\r\n",
    "    \r\n",
    "    ax[0, 0].legend(loc='lower left')\r\n",
    "    fig.tight_layout()\r\n",
    "\r\n",
    "get_plots_by_gas(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_corr(df_dict: Dict[str, pd.DataFrame]):\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        current_ax = ax[i // 2, i % 2]\r\n",
    "        sns.heatmap(data=df_dict[gas].corr().abs(), annot=True, cmap='bone_r', vmin=0.0, vmax=1.0, ax=current_ax)\r\n",
    "        current_ax.set_title(gas)\r\n",
    "\r\n",
    "get_corr(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### After Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_partitioned, boundaries = get_df_partitioned_by_gas(df=df, clean_columns=['T', 'RH', 'Value'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_exog_variable_distributions(df=df_partitioned['CO'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_gas_value_distributions(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_general_trends(df_dict=df_partitioned, ma_windows=[1, 7, 30], kind='ma')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_plots_by_gas(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_corr(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyper-Parameter Tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tools"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cf_summary(df: pd.DataFrame, kind: Union['pacf', 'acf'], threshold: float = 0.01, max_lag: int = 20, plot: bool = False):\r\n",
    "    corr_fn = pacf if kind == 'pacf' else acf\r\n",
    "    plot_fn = plot_pacf if kind == 'pacf' else plot_acf\r\n",
    "\r\n",
    "    lag_space = min(max_lag, df.count() // 2 - 1)\r\n",
    "    corr_scores, conf_intvs = corr_fn(df, nlags=lag_space, alpha=threshold)\r\n",
    "\r\n",
    "    lower_conf_bound = conf_intvs[:, 0] - corr_scores\r\n",
    "    upper_conf_bound = conf_intvs[:, 1] - corr_scores\r\n",
    "\r\n",
    "    lag_values = np.where((corr_scores < lower_conf_bound) | (corr_scores > upper_conf_bound))[0][1:]\r\n",
    "    best_t = lag_values[np.argmax((np.abs(corr_scores[lag_values])))] if len(lag_values) > 0 else 0\r\n",
    "\r\n",
    "    if plot:\r\n",
    "        ax = plot_fn(df, lags=lag_space, alpha=threshold, zero=False).get_axes()[0]\r\n",
    "        ax.set_title(f'Significant lag values: {tuple(lag_values)} Best lag: {best_t}')\r\n",
    "        ax.set_xticks([i for i in range(1, lag_space + 1)])\r\n",
    "\r\n",
    "    return lag_values, best_t\r\n",
    "\r\n",
    "cf_summary(df_partitioned['CO']['Value'], kind='pacf', threshold=0.01, plot=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_seasonal_periods(df_dict: Dict[str, pd.DataFrame], plot: bool = False):\r\n",
    "    m_dict = {}\r\n",
    "    if plot:\r\n",
    "        fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        df = df_dict[gas]\r\n",
    "        seasonal_component = seasonal_decompose(df['Value']).seasonal\r\n",
    "        _, m = cf_summary(seasonal_component, kind='acf')\r\n",
    "        m_dict[gas] = m\r\n",
    "        if plot:\r\n",
    "            focus = seasonal_component['2004-04-01':'2004-04-20']\r\n",
    "            current_ax = ax[i // 2, i % 2]\r\n",
    "            current_ax.plot(focus.index.day, focus)\r\n",
    "            current_ax.set_xticks(focus.index.day)\r\n",
    "            current_ax.set_title(gas)\r\n",
    "    return m_dict\r\n",
    "\r\n",
    "get_seasonal_periods(df_partitioned, plot=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_param_summary(df_dict: Dict[str, pd.DataFrame], threshold: float = 0.01, plot_differenced_values: bool = False):\r\n",
    "    param_summary = {}\r\n",
    "    if plot_differenced_values:\r\n",
    "        fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 6))\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        df = df_dict[gas]\r\n",
    "\r\n",
    "        is_stationary = adfuller(df['Value'])[1] < threshold\r\n",
    "\r\n",
    "        differenced_values = diff(df['Value'], k_diff=0 if is_stationary else 1, k_seasonal_diff=1, seasonal_periods=7)\r\n",
    "\r\n",
    "        p_values, best_p = cf_summary(differenced_values, kind='pacf', max_lag=6, threshold=threshold)\r\n",
    "        q_values, best_q = cf_summary(differenced_values, kind='acf', max_lag=6, threshold=threshold)\r\n",
    "\r\n",
    "        param_summary[gas] = {\r\n",
    "            'p_values': p_values,\r\n",
    "            'best_p': best_p,\r\n",
    "            'd_value': 0 if is_stationary else 1,\r\n",
    "            'q_values': q_values,\r\n",
    "            'best_q': best_q\r\n",
    "        }\r\n",
    "\r\n",
    "        if plot_differenced_values:\r\n",
    "            current_ax = ax[i // 2, i % 2]\r\n",
    "            current_ax.plot(differenced_values)\r\n",
    "            current_ax.set_title(df['Gas'][0])\r\n",
    "            current_ax.set_xticks([])\r\n",
    "            current_ax.set_xlabel(xlabel='Differenced Values')\r\n",
    "        \r\n",
    "    return param_summary\r\n",
    "\r\n",
    "get_param_summary(df_partitioned, threshold=0.01, plot_differenced_values=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def format_selective_order(l: List[int]):\r\n",
    "    list_tmp = [0 for _ in range(max(l))]\r\n",
    "    for i in l:\r\n",
    "        list_tmp[i - 1] = 1\r\n",
    "    return tuple(list_tmp)\r\n",
    "\r\n",
    "format_selective_order([1, 6])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_params(df_dict: Dict[str, pd.DataFrame], threshold: float = 0.01):\r\n",
    "    param_summary = get_param_summary(df_dict, threshold=threshold)\r\n",
    "    orders = {}\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        p = format_selective_order(param_summary[gas]['p_values'])\r\n",
    "        d = param_summary[gas]['d_value']\r\n",
    "        q = format_selective_order(param_summary[gas]['q_values'])\r\n",
    "        orders[gas] = (p, d, q)\r\n",
    "    return orders\r\n",
    "\r\n",
    "get_params(df_dict=df_partitioned)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid Searches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate_params(df: pd.DataFrame, algo: Union['sarima', 'sarimax', 'expsmth'], init_kws: Dict, fit_kws: Dict, training_size: float = 0.75):\r\n",
    "    from sklearn.metrics import mean_squared_error\r\n",
    "    with warnings.catch_warnings():\r\n",
    "        warnings.simplefilter('ignore', ConvergenceWarning)\r\n",
    "\r\n",
    "        # Train-Test Split\r\n",
    "        partition_index = int(df.shape[0] * training_size)\r\n",
    "\r\n",
    "        train = df.iloc[:partition_index,:]\r\n",
    "        test = df.iloc[partition_index:,:]\r\n",
    "\r\n",
    "        if algo == 'sarima':\r\n",
    "            model = SARIMAX(endog=train['Value'], **init_kws).fit(**fit_kws)\r\n",
    "            train_pred = model.predict(start=0, end=partition_index - 1)\r\n",
    "            test_pred = model.forecast(steps=df.shape[0] - partition_index)\r\n",
    "        elif algo == 'sarimax':\r\n",
    "            exog = init_kws.pop('exog')\r\n",
    "            model = SARIMAX(endog=train['Value'], exog=train[exog], **init_kws).fit(**fit_kws)\r\n",
    "            train_pred = model.predict(start=0, end=partition_index - 1, exog=train[exog])\r\n",
    "            test_pred = model.forecast(steps=df.shape[0] - partition_index, exog=test[exog])\r\n",
    "            init_kws['exog'] = exog\r\n",
    "        elif algo == 'expsmth':\r\n",
    "            model = ExponentialSmoothing(endog=train['Value'], **init_kws).fit(**fit_kws)\r\n",
    "            train_pred = model.predict(start=0, end=partition_index - 1)\r\n",
    "            test_pred = model.forecast(steps=df.shape[0] - partition_index)\r\n",
    "\r\n",
    "        return {\r\n",
    "            'aic': model.aic,\r\n",
    "            'train_rmse': mean_squared_error(train['Value'], train_pred, squared=False),\r\n",
    "            'test_rmse': mean_squared_error(test['Value'], test_pred, squared=False)\r\n",
    "        }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate_grid_search(gs_results: Dict, weights: Tuple[int, int, int] = (0.5, 0.4, 0.1)):\r\n",
    "    gs_best_params = {}\r\n",
    "    for gas in gs_results.keys():\r\n",
    "        current_results = gs_results[gas]\r\n",
    "\r\n",
    "        score_fn = lambda x: weights[0] * x['test_rmse'] + weights[1] * x['train_rmse'] + weights[2] * x['aic']\r\n",
    "\r\n",
    "        scores = list(map(score_fn, current_results))\r\n",
    "        index = np.argmin(scores)\r\n",
    "\r\n",
    "        best_params = current_results[index]\r\n",
    "        gs_best_params[gas] = {\r\n",
    "            'params': best_params['params'],\r\n",
    "            'score': scores[index]\r\n",
    "        }\r\n",
    "    return gs_best_params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Grid Search: SARIMA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def grid_search_sarima(df_dict: Dict[str, pd.DataFrame]):\r\n",
    "    from numpy.linalg import LinAlgError\r\n",
    "    gs_summary = {\r\n",
    "        'CO': [],\r\n",
    "        'NOx': [],\r\n",
    "        'NMHC': [],\r\n",
    "        'O3': []\r\n",
    "    }\r\n",
    "    seasonal_params = {\r\n",
    "        'P_space': [0, 1, 2],\r\n",
    "        'D_space': [0, 1],\r\n",
    "        'Q_space': [0, 1, 2]\r\n",
    "    }\r\n",
    "    seasonal_periods = get_seasonal_periods(df_dict=df_dict)\r\n",
    "    trend_orders = get_params(df_dict=df_dict, threshold=0.01)\r\n",
    "    for gas in df_dict.keys():\r\n",
    "        T = trend_orders[gas]\r\n",
    "        m = seasonal_periods[gas]\r\n",
    "        df = df_dict[gas]\r\n",
    "        for trend in ['n', 'c']:\r\n",
    "            for P in seasonal_params['P_space']:\r\n",
    "                for D in seasonal_params['D_space']:\r\n",
    "                    for Q in seasonal_params['Q_space']:\r\n",
    "                        try:\r\n",
    "                            results = evaluate_params(\r\n",
    "                                df=df,\r\n",
    "                                algo='sarima',\r\n",
    "                                init_kws={\r\n",
    "                                    'order': T,\r\n",
    "                                    'seasonal_order': (P, D, Q, m),\r\n",
    "                                    'trend': trend\r\n",
    "                                },\r\n",
    "                                fit_kws={}\r\n",
    "                            )\r\n",
    "                            gs_summary[gas].append({\r\n",
    "                                'params': {\r\n",
    "                                    'init': {\r\n",
    "                                        'order': T,\r\n",
    "                                        'seasonal_order': (P, D, Q, m),\r\n",
    "                                        'trend': trend\r\n",
    "                                    },\r\n",
    "                                    'fit': {}\r\n",
    "                                },\r\n",
    "                                'aic': results['aic'],\r\n",
    "                                'train_rmse': results['train_rmse'],\r\n",
    "                                'test_rmse': results['test_rmse']\r\n",
    "                            })\r\n",
    "                        except LinAlgError:\r\n",
    "                            continue\r\n",
    "                        except ValueError:\r\n",
    "                            continue\r\n",
    "    return gs_summary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the model\r\n",
    "# gs_sarima = grid_search_sarima(df_dict=df_partitioned)\r\n",
    "# pickle.dump(obj=gs_sarima, file=open('./tmp/models/gs-sarima-results.p', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the model\r\n",
    "gs_sarima = pickle.load(file=open('./tmp/models/gs-sarima-results.p', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find best parameters for SARIMA\r\n",
    "best_sarima = evaluate_grid_search(gs_results=gs_sarima, weights=(2, 1, 0.05))\r\n",
    "best_sarima"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Grid Search: SARIMAX"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def grid_search_sarimax_1(df_dict: Dict[str, pd.DataFrame]):\r\n",
    "    from numpy.linalg import LinAlgError\r\n",
    "    gs_summary = {\r\n",
    "        'CO': [],\r\n",
    "        'NOx': [],\r\n",
    "        'NMHC': [],\r\n",
    "        'O3': []\r\n",
    "    }\r\n",
    "    seasonal_params = {\r\n",
    "        'P_space': [0, 1, 2],\r\n",
    "        'D_space': [0, 1],\r\n",
    "        'Q_space': [0, 1, 2]\r\n",
    "    }\r\n",
    "    seasonal_periods = get_seasonal_periods(df_dict=df_dict)\r\n",
    "    trend_orders = get_params(df_dict=df_dict, threshold=0.01)\r\n",
    "    for gas in df_dict.keys():\r\n",
    "        T = trend_orders[gas]\r\n",
    "        m = seasonal_periods[gas]\r\n",
    "        df = df_dict[gas]\r\n",
    "        for trend in ['n', 'c']:\r\n",
    "            for P in seasonal_params['P_space']:\r\n",
    "                for D in seasonal_params['D_space']:\r\n",
    "                    for Q in seasonal_params['Q_space']:\r\n",
    "                        try:\r\n",
    "                            results = evaluate_params(\r\n",
    "                                df=df,\r\n",
    "                                algo='sarimax',\r\n",
    "                                init_kws={\r\n",
    "                                    'order': T,\r\n",
    "                                    'seasonal_order': (P, D, Q, m),\r\n",
    "                                    'trend': trend,\r\n",
    "                                    'exog': ['RH', 'T']\r\n",
    "                                }, fit_kws={}\r\n",
    "                            )\r\n",
    "                            gs_summary[gas].append({\r\n",
    "                                'params': {\r\n",
    "                                    'init': {\r\n",
    "                                        'order': T,\r\n",
    "                                        'seasonal_order': (P, D, Q, m),\r\n",
    "                                        'trend': trend,\r\n",
    "                                        'exog': ['RH', 'T']\r\n",
    "                                    },\r\n",
    "                                    'fit': {}\r\n",
    "                                },\r\n",
    "                                'aic': results['aic'],\r\n",
    "                                'train_rmse': results['train_rmse'],\r\n",
    "                                'test_rmse': results['test_rmse']\r\n",
    "                            })\r\n",
    "                        except LinAlgError:\r\n",
    "                            continue\r\n",
    "                        except ValueError:\r\n",
    "                            continue\r\n",
    "    return gs_summary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the model\r\n",
    "# gs_sarimax_1 = grid_search_sarimax_1(df_dict=df_partitioned)\r\n",
    "# pickle.dump(obj=gs_sarimax_1, file=open('./tmp/models/gs-sarimax-1-results.p', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the model\r\n",
    "gs_sarimax_1 = pickle.load(file=open('./tmp/models/gs-sarimax-1-results.p', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find best parameters for SARIMA of SARIMAX\r\n",
    "best_sarimax_1 = evaluate_grid_search(gs_results=gs_sarimax_1, weights=(2, 1, 0.05))\r\n",
    "best_sarimax_1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def grid_search_sarimax_2(df_dict: Dict[str, pd.DataFrame], best_sarimax_1: Dict):\r\n",
    "    gs_summary = {\r\n",
    "        'CO': [],\r\n",
    "        'NOx': [],\r\n",
    "        'NMHC': [],\r\n",
    "        'O3': []\r\n",
    "    }\r\n",
    "    for gas in df_dict.keys():\r\n",
    "        df = df_dict[gas]\r\n",
    "        current_best_sarimax_1 = best_sarimax_1[gas]\r\n",
    "        default_exog = current_best_sarimax_1['params']['init'].pop('exog')\r\n",
    "        for exog in ['T', 'RH', ['T', 'RH']]:\r\n",
    "            try:\r\n",
    "                results = evaluate_params(\r\n",
    "                    df=df,\r\n",
    "                    algo='sarimax',\r\n",
    "                    init_kws={\r\n",
    "                        **current_best_sarimax_1['params']['init'],\r\n",
    "                        'exog': exog\r\n",
    "                    },\r\n",
    "                    fit_kws={}\r\n",
    "                )\r\n",
    "                gs_summary[gas].append({\r\n",
    "                    'params': {\r\n",
    "                        'init': {\r\n",
    "                            **current_best_sarimax_1['params']['init'],\r\n",
    "                            'exog': exog\r\n",
    "                        },\r\n",
    "                        'fit': {}\r\n",
    "                    },\r\n",
    "                    'aic': results['aic'],\r\n",
    "                    'train_rmse': results['train_rmse'],\r\n",
    "                    'test_rmse': results['test_rmse']\r\n",
    "                })\r\n",
    "            except LinAlgError:\r\n",
    "                continue\r\n",
    "        current_best_sarimax_1['params']['init'] = default_exog\r\n",
    "    return gs_summary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the model\r\n",
    "# gs_sarimax_2 = grid_search_sarimax_2(df_dict=df_partitioned, best_sarimax_1=best_sarimax_1)\r\n",
    "# pickle.dump(obj=gs_sarimax_2, file=open('./tmp/models/gs-sarimax-2-results.p', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the model\r\n",
    "gs_sarimax_2 = pickle.load(file=open('./tmp/models/gs-sarimax-2-results.p', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find the best parameters for SARIMAX\r\n",
    "best_sarimax = evaluate_grid_search(gs_results=gs_sarimax_2, weights=(2, 1, 0.05))\r\n",
    "best_sarimax"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Grid Search: Exponential Smoothing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def grid_search_exp_smth(df_dict: Dict[str, pd.DataFrame]):\r\n",
    "    gs_summary = {\r\n",
    "        'CO': [],\r\n",
    "        'NOx': [],\r\n",
    "        'NMHC': [],\r\n",
    "        'O3': []\r\n",
    "    }\r\n",
    "    trend_types = ['add', 'mul']\r\n",
    "    seasonal_types = ['add', 'mul']\r\n",
    "\r\n",
    "    seasonal_periods = get_seasonal_periods(df_dict=df_dict)\r\n",
    "    smoothing_levels = np.linspace(0.01, 0.9, 8)\r\n",
    "\r\n",
    "    for gas in df_dict.keys():\r\n",
    "        for smoothing_level in smoothing_levels:\r\n",
    "            for t in trend_types:\r\n",
    "                for s in seasonal_types:\r\n",
    "                    df = df_dict[gas]\r\n",
    "                    try:\r\n",
    "                        results = evaluate_params(\r\n",
    "                            df=df,\r\n",
    "                            algo='expsmth',\r\n",
    "                            init_kws={\r\n",
    "                                'trend': t,\r\n",
    "                                'seasonal': s,\r\n",
    "                                'seasonal_periods': seasonal_periods[gas]\r\n",
    "                            },\r\n",
    "                            fit_kws={\r\n",
    "                                'smoothing_level': smoothing_level\r\n",
    "                            }\r\n",
    "                        )\r\n",
    "                        gs_summary[gas].append({\r\n",
    "                            'params': {\r\n",
    "                                'init': {\r\n",
    "                                    'trend': t,\r\n",
    "                                    'seasonal': s,\r\n",
    "                                    'seasonal_periods': seasonal_periods[gas]\r\n",
    "                                },\r\n",
    "                                'fit': {\r\n",
    "                                    'smoothing_level': smoothing_level\r\n",
    "                                }\r\n",
    "                            },\r\n",
    "                            'aic': results['aic'],\r\n",
    "                            'train_rmse': results['train_rmse'],\r\n",
    "                            'test_rmse': results['test_rmse']\r\n",
    "                        })\r\n",
    "                    except LinAlgError:\r\n",
    "                        continue\r\n",
    "                    except ValueError:\r\n",
    "                        continue\r\n",
    "    return gs_summary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the model\r\n",
    "# gs_expsmth = grid_search_exp_smth(df_dict=df_partitioned)\r\n",
    "# pickle.dump(obj=gs_expsmth, file=open('./tmp/models/gs-expsmth-results.p', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the model\r\n",
    "gs_expsmth = pickle.load(file=open('./tmp/models/gs-expsmth-results.p', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find the best parameters for ExponentialSmoothing model\r\n",
    "best_expsmth = evaluate_grid_search(gs_results=gs_expsmth, weights=(2, 1, 0.05))\r\n",
    "best_expsmth"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compare_algorithms(best_sarima: Dict[str, Dict], best_sarimax: Dict[str, Dict], best_expsmth: Dict[str, Dict]):\r\n",
    "    best_algorithms = {}\r\n",
    "    for gas in ['CO', 'NOx', 'NMHC', 'O3']:\r\n",
    "        current_best_sarima = best_sarima[gas]\r\n",
    "        current_best_sarimax = best_sarimax[gas]\r\n",
    "        current_best_expsmth = best_expsmth[gas]\r\n",
    "\r\n",
    "        sarima_score = current_best_sarima['score']\r\n",
    "        sarimax_score = current_best_sarimax['score']\r\n",
    "        expsmth_score = current_best_expsmth['score']\r\n",
    "\r\n",
    "        best_model = ['sarima', 'sarimax', 'expsmth'][np.argmin([sarima_score, sarimax_score, expsmth_score])]\r\n",
    "\r\n",
    "        if best_model == 'sarima':\r\n",
    "            model = SARIMAX\r\n",
    "            init = current_best_sarima['params']['init']\r\n",
    "            fit = current_best_sarima['params']['fit']\r\n",
    "        elif best_model == 'sarimax':\r\n",
    "            model = SARIMAX\r\n",
    "            init = current_best_sarimax['params']['init']\r\n",
    "            fit = current_best_sarimax['params']['fit']\r\n",
    "        elif best_model == 'expsmth':\r\n",
    "            model = ExponentialSmoothing\r\n",
    "            init = current_best_expsmth['params']['init']\r\n",
    "            fit = current_best_expsmth['params']['fit']\r\n",
    "\r\n",
    "        best_algorithms[gas] = {\r\n",
    "            'model': model[0] if type(model) is tuple else model,\r\n",
    "            'init': init,\r\n",
    "            'fit': fit\r\n",
    "        }\r\n",
    "    return best_algorithms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the results\r\n",
    "# best_models = compare_algorithms(best_sarima, best_sarimax, best_expsmth)\r\n",
    "# pickle.dump(obj=best_models, file=open('./tmp/models/best-models.p', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_models = pickle.load(file=open('./tmp/models/best-models.p', 'rb'))\r\n",
    "best_models"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making Predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating In-Sample Predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "def evaluate_in_sample_predictions(df_dict: Dict[str, pd.DataFrame], models: Dict[str, Dict], training_size: float = 0.75):\r\n",
    "    with warnings.catch_warnings():\r\n",
    "        warnings.simplefilter('ignore', ConvergenceWarning)\r\n",
    "        results = {}\r\n",
    "        for gas in df_dict.keys():\r\n",
    "            current_config = models[gas]\r\n",
    "            algo = current_config['model']\r\n",
    "\r\n",
    "            init_params = current_config['init']\r\n",
    "            fit_params = current_config['fit']\r\n",
    "\r\n",
    "            df = df_dict[gas]\r\n",
    "            partition_index = int(df.shape[0] * training_size)\r\n",
    "\r\n",
    "            train = df[:partition_index]\r\n",
    "            test = df[partition_index:]\r\n",
    "\r\n",
    "            if 'exog' in init_params:\r\n",
    "                exog = init_params.pop('exog')\r\n",
    "                model = algo(endog=train['Value'], exog=train[exog], **init_params).fit(**fit_params)\r\n",
    "                train_pred = model.predict(start=0, end=partition_index - 1, exog=train[exog])\r\n",
    "                test_pred = model.forecast(steps=df.shape[0] - partition_index, exog=test[exog])\r\n",
    "                init_params['exog'] = exog\r\n",
    "            else:\r\n",
    "                model = algo(endog=train['Value'], **init_params).fit(**fit_params)\r\n",
    "                train_pred = model.predict(start=0, end=partition_index - 1)\r\n",
    "                test_pred = model.forecast(steps=df.shape[0] - partition_index)\r\n",
    "\r\n",
    "            train_err = mean_squared_error(train['Value'], train_pred, squared=False)\r\n",
    "            test_err = mean_squared_error(test['Value'], test_pred, squared=False)\r\n",
    "\r\n",
    "            results[gas] = {\r\n",
    "                'aic': model.aic,\r\n",
    "                'train_true': train['Value'],\r\n",
    "                'train_pred': train_pred,\r\n",
    "                'train_rmse': train_err,\r\n",
    "                'test_true': test['Value'],\r\n",
    "                'test_pred': test_pred,\r\n",
    "                'test_rmse': test_err\r\n",
    "            }\r\n",
    "        return results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the results\r\n",
    "# evaluation_results = evaluate_in_sample_predictions(df_dict=df_partitioned, models=best_models)\r\n",
    "# pickle.dump(obj=evaluation_results, file=open('./tmp/models/evaluation-results.p', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "evaluation_results = pickle.load(file=open('./tmp/models/evaluation-results.p', 'rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def decompose(results: Dict[str, Dict]):\r\n",
    "    aic_values = []\r\n",
    "    train_rmse_values = []\r\n",
    "    test_rmse_values = []\r\n",
    "\r\n",
    "    for gas in results.keys():\r\n",
    "        aic_values.append(results[gas]['aic'])\r\n",
    "        train_rmse_values.append(results[gas]['train_rmse'])\r\n",
    "        test_rmse_values.append(results[gas]['test_rmse'])\r\n",
    "    decomposed_results = {\r\n",
    "        'mean_aic': sum(aic_values) / len(aic_values),\r\n",
    "        'mean_train_rmse': sum(train_rmse_values) / len(train_rmse_values),\r\n",
    "        'mean_test_rmse': sum(test_rmse_values) / len(test_rmse_values),\r\n",
    "    }\r\n",
    "    return decomposed_results\r\n",
    "\r\n",
    "decompose(results=evaluation_results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_predictions(results: Dict[str, Dict]):\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 7))\r\n",
    "    for i, gas in enumerate(results.keys()):\r\n",
    "        current_results = results[gas]\r\n",
    "        current_ax = ax[i // 2, i % 2]\r\n",
    "        current_ax.plot(pd.concat(objs=(current_results['train_true'], current_results['test_true']), axis=0))\r\n",
    "        current_ax.plot(current_results['train_pred'])\r\n",
    "        current_ax.plot(current_results['test_pred'])\r\n",
    "        current_ax.set_title(gas)\r\n",
    "\r\n",
    "plot_predictions(results=evaluation_results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating Out-of-Sample Predictions (Forecasts)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_test_df(file_path: str):\r\n",
    "    test_df = pd.read_csv(filepath_or_buffer=file_path, sep=',', header=0)\r\n",
    "    test_df['Date'] = pd.to_datetime(test_df['Date'], format='%d/%m/%Y')\r\n",
    "    test_df.set_index(keys='Date', inplace=True)\r\n",
    "    return test_df.sort_index(ascending=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clean_and_partition_test_data(file_path: str = './data/test.csv'):\r\n",
    "    test_df = get_test_df(file_path=file_path)\r\n",
    "    test_df_partitioned = {}\r\n",
    "    for gas in test_df['Gas'].unique():\r\n",
    "        test_df_partitioned[gas] = DataCleaner(boundaries=boundaries[gas]).fit_transform(test_df[test_df['Gas'] == gas])\r\n",
    "    return test_df_partitioned"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate_out_of_sample_predictions(df_dict: Dict[str, pd.DataFrame], params: Dict[str, Dict], test_file: str, out_file: str, segment_start: float, segment_end: float):\r\n",
    "    test_df_dict = clean_and_partition_test_data(file_path=test_file)\r\n",
    "    results = []\r\n",
    "    with warnings.catch_warnings():\r\n",
    "        warnings.simplefilter('ignore', ConvergenceWarning)\r\n",
    "        for gas in df_dict.keys():\r\n",
    "            df = df_dict[gas]\r\n",
    "            target = test_df_dict[gas]\r\n",
    "            \r\n",
    "            partition_index_1 = int(df.shape[0] * segment_start)\r\n",
    "            partition_index_2 = int(df.shape[0] * segment_end)\r\n",
    "\r\n",
    "            train = df[partition_index_1:partition_index_2]\r\n",
    "            padding = df[partition_index_2:]\r\n",
    "\r\n",
    "            current_config = params[gas]\r\n",
    "            algo = current_config['model']\r\n",
    "            init_params = current_config['init']\r\n",
    "            fit_params = current_config['fit']\r\n",
    "\r\n",
    "            print(f'Init Parameters:\\t{init_params}')\r\n",
    "            print(f'Fit Parameters:\\t{fit_params}')\r\n",
    "\r\n",
    "            if 'exog' in init_params.keys():\r\n",
    "                exog = init_params.pop('exog')\r\n",
    "                model = algo(endog=train['Value'], exog=train[exog], **init_params).fit(**fit_params)\r\n",
    "                pred = model.forecast(\r\n",
    "                    steps=padding.shape[0] + target.shape[0],\r\n",
    "                    exog=pd.concat(objs=(padding[exog], target[exog]), axis=0)\r\n",
    "                ).rename('Value')\r\n",
    "                init_params['exog'] = exog\r\n",
    "            else:\r\n",
    "                model = algo(endog=train['Value'], **init_params).fit(**fit_params)\r\n",
    "                pred = model.forecast(\r\n",
    "                    steps=padding.shape[0] + target.shape[0]\r\n",
    "                ).rename('Value')\r\n",
    "\r\n",
    "            result = pd.merge(left=pred, right=target, left_index=True, right_index=True, how='inner')\r\n",
    "\r\n",
    "            print(f'{result.shape[0]} steps forecasted', end='\\n\\n')\r\n",
    "\r\n",
    "            results.append(result.set_index(keys='id')['Value'])\r\n",
    "    \r\n",
    "    predictions = pd.concat(objs=results, axis=0)\r\n",
    "    predictions.sort_index(inplace=True)\r\n",
    "    predictions.to_csv(out_file, sep=',', index_label='id')\r\n",
    "\r\n",
    "pred_df = evaluate_out_of_sample_predictions(\r\n",
    "    df_dict=df_partitioned,\r\n",
    "    segment_start=0.0,\r\n",
    "    segment_end=0.75,\r\n",
    "    params=best_models,\r\n",
    "    test_file='./data/test.csv',\r\n",
    "    out_file='./out/clean.csv'\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_forecasts(df_dict: Dict[str, pd.DataFrame], prediction_file: str):\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\r\n",
    "    test_df_dict = clean_and_partition_test_data()\r\n",
    "    pred_df = pd.read_csv(prediction_file, index_col=0)\r\n",
    "    for i, gas in enumerate(df_dict.keys()):\r\n",
    "        df = df_dict[gas]\r\n",
    "        target = test_df_dict[gas]\r\n",
    "        future = pd.merge(left=target, right=pred_df, left_on='id', right_index=True, how='inner')\r\n",
    "        \r\n",
    "        current_ax = ax[i // 2, i % 2]\r\n",
    "        ax_twin = current_ax.twinx()\r\n",
    "\r\n",
    "        current_ax.plot(df['Value'])\r\n",
    "        current_ax.plot(future['Value'])\r\n",
    "        current_ax.set_title(gas)\r\n",
    "\r\n",
    "plot_forecasts(df_dict=df_partitioned, prediction_file='./out/clean.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Baseline Prediction (constant model)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# baseline prediction, since they are all stationary\r\n",
    "# test_set = pd.read_csv(filepath_or_buffer='./data/test.csv', sep=',')\r\n",
    "# pd.merge(\r\n",
    "#     left=test_set,\r\n",
    "#     right=df.groupby(by='Gas').mean(),\r\n",
    "#     left_on='Gas',\r\n",
    "#     right_index=True,\r\n",
    "#     how='inner'\r\n",
    "# )[['id', 'Value']].set_index(keys='id').to_csv('./out/baseline.csv', sep=',', index_label='id')\r\n",
    "plot_forecasts(df_dict=df_partitioned, prediction_file='./out/baseline.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part B > Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Exclusive Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preprocessing Dependencies\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "# Clustering Algorithms\r\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS, AffinityPropagation, AgglomerativeClustering\r\n",
    "\r\n",
    "# Clustering metrics\r\n",
    "from sklearn.metrics import silhouette_score\r\n",
    "import plotly.io\r\n",
    "plotly.io.renderers.default='notebook'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2 = pd.read_csv('./data/Mall_Customers.csv', index_col=0)\r\n",
    "df2.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rename_columns(df: pd.DataFrame):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    df_tmp.rename(mapper={\r\n",
    "        'Genre': 'Gender',\r\n",
    "        'Annual Income (k$)': 'Annual Income',\r\n",
    "        'Spending Score (1-100)': 'Spending Score'\r\n",
    "    }, axis=1, inplace=True)\r\n",
    "    # df_tmp['Annual Income'] = df_tmp['Annual Income'] * 1000\r\n",
    "    return df_tmp\r\n",
    "\r\n",
    "df2 = rename_columns(df2)\r\n",
    "df2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_distribution_of_each_variable(df: pd.DataFrame):\r\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\r\n",
    "    for i, c in enumerate(df.columns):\r\n",
    "        var = df[c]\r\n",
    "        kde = var.dtype.kind in 'biufc'\r\n",
    "        sns.histplot(var, kde=kde, ax=ax[i // 2, i % 2])\r\n",
    "\r\n",
    "get_distribution_of_each_variable(df2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_distribution_by_gender(df: pd.DataFrame):\r\n",
    "    fig, ax = plt.subplots(nrows=3, figsize=(6, 6))\r\n",
    "    for i, c in enumerate(['Age', 'Annual Income', 'Spending Score']):\r\n",
    "        var = df[c]\r\n",
    "        sns.kdeplot(x=var, hue=df['Gender'], ax=ax[i])\r\n",
    "    fig.tight_layout()\r\n",
    "\r\n",
    "get_distribution_by_gender(df2)\r\n",
    "\r\n",
    "# It seems that there are slightly more data on female customers than male ones"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_corr_heatmap(df: pd.DataFrame):\r\n",
    "    sns.heatmap(df.corr(), annot=True, vmin=-1.0, vmax=1.0, cmap='RdBu').set_title('Correlations between Quantitative Features')\r\n",
    "\r\n",
    "get_corr_heatmap(df2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_gender_importance(df: pd.DataFrame):\r\n",
    "    fig, ax = plt.subplots(nrows=2, figsize=(5, 5))\r\n",
    "    sns.scatterplot(data=df, x='Annual Income', y='Spending Score', hue='Gender', ax=ax[0])\r\n",
    "    sns.scatterplot(data=df, x='Annual Income', y='Age', hue='Gender', ax=ax[1])\r\n",
    "    fig.tight_layout()\r\n",
    "\r\n",
    "get_gender_importance(df=df2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Engineering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Gender column is dropped as it is not useful in solving this clustering problem\r\n",
    "df2.drop(columns='Gender', inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def standardize_variables(df: pd.DataFrame, cols: List[str]):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    df_tmp[cols] = StandardScaler().fit_transform(df_tmp[cols])\r\n",
    "    return df_tmp\r\n",
    "\r\n",
    "df2_s = standardize_variables(df2, cols=['Age', 'Annual Income', 'Spending Score'])\r\n",
    "df2_s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_distribution_of_each_variable(df=df2_s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Principal Component Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from typing import Union, List\r\n",
    "# plus logic\r\n",
    "def get_pca_results(df: pd.DataFrame):\r\n",
    "    df_scaled = StandardScaler().fit_transform(X=df)\r\n",
    "\r\n",
    "    pca = PCA(n_components=df_scaled.shape[1]).fit(X=df_scaled)\r\n",
    "    header = ['Eigenvalue', 'Explained Variance', 'Cumulative Explained Variance']\r\n",
    "    header.extend(df.columns.tolist())\r\n",
    "    eigenvalues = pca.explained_variance_\r\n",
    "    eigenvectors = pca.components_\r\n",
    "    expl_var = pca.explained_variance_ratio_\r\n",
    "    cum_expl_var = pca.explained_variance_ratio_.cumsum()\r\n",
    "    pca_results = pd.DataFrame(\r\n",
    "        data=np.hstack((\r\n",
    "            eigenvalues.reshape(-1, 1),\r\n",
    "            expl_var.reshape(-1, 1),\r\n",
    "            cum_expl_var.reshape(-1, 1),\r\n",
    "            eigenvectors\r\n",
    "        )),\r\n",
    "        columns=header,\r\n",
    "        index=[f'PC {i + 1}' for i in range(df_scaled.shape[1])]\r\n",
    "    )\r\n",
    "\r\n",
    "    df_transformed = pd.DataFrame(\r\n",
    "        data=pca.transform(df_scaled),\r\n",
    "        index=df.index,\r\n",
    "        columns=[f'PC {i + 1}' for i in range(df_scaled.shape[1])]\r\n",
    "    )\r\n",
    "\r\n",
    "    return pca_results, df_transformed\r\n",
    "\r\n",
    "pca_results, df2_transformed = get_pca_results(df=df2_s)\r\n",
    "pca_results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scree_plot(df: pd.DataFrame, pca: pd.DataFrame):\r\n",
    "    with sns.axes_style(style='darkgrid'):\r\n",
    "        ax = sns.pointplot(data=pca, x=pca.index, y=pca['Eigenvalue'])\r\n",
    "        ax.set(\r\n",
    "            title='Scree Plot for PCA (df2)',\r\n",
    "            ylim=(0, 1.4)\r\n",
    "        )\r\n",
    "        ax.annotate(text='As there is no elbow,\\nno PC should be discarded', xy=(1.75, 1.2), ha='center')\r\n",
    "        return ax\r\n",
    "\r\n",
    "scree_plot(df2, pca_results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visual Analysis - Compare Algorithms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KMEANS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def kmeans_preview(df: pd.DataFrame, clusters: List[int] = [3, 4, 5]):\r\n",
    "    for c in clusters:\r\n",
    "        model = KMeans(n_clusters=c)\r\n",
    "        data = df[['Annual Income', 'Age', 'Spending Score']]\r\n",
    "        y_hat = model.fit_predict(X=data).astype(str)\r\n",
    "        label = f'KMeans: {c} clusters: {silhouette_score(X=data, labels=y_hat):.2f}'\r\n",
    "        fig = px.scatter_3d(data_frame=df, x='Annual Income', y='Age', z='Spending Score', color=y_hat, title=label)\r\n",
    "        fig.show()\r\n",
    "\r\n",
    "kmeans_preview(df=df2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dbscan_preview(df: pd.DataFrame, epsilon_space = np.linspace(10, 20, 3)):\r\n",
    "    for e in epsilon_space:\r\n",
    "        model = DBSCAN(eps=e, min_samples=10)\r\n",
    "        data = df[['Annual Income', 'Age', 'Spending Score']]\r\n",
    "        y_hat = model.fit_predict(X=data).astype(str)\r\n",
    "        label = f'DBSCAN: epsilon={e}: {silhouette_score(X=data, labels=y_hat):.2f}'\r\n",
    "        fig = px.scatter_3d(data_frame=df, x='Annual Income', y='Age', z='Spending Score', color=y_hat, title=label)\r\n",
    "        fig.show()\r\n",
    "\r\n",
    "dbscan_preview(df=df2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def optics_preview(df: pd.DataFrame, max_eps_space = np.linspace(10, 20, 3)):\r\n",
    "    for e in max_eps_space:\r\n",
    "        model = OPTICS(max_eps=e, min_samples=10)\r\n",
    "        data = df[['Annual Income', 'Age', 'Spending Score']]\r\n",
    "        y_hat = model.fit_predict(X=data).astype(str)\r\n",
    "        label = f'OPTICS: max epsilon={e}: {silhouette_score(X=data, labels=y_hat):.2f}'\r\n",
    "        fig = px.scatter_3d(data_frame=df, x='Annual Income', y='Age', z='Spending Score', color=y_hat, title=label)\r\n",
    "        fig.show()\r\n",
    "\r\n",
    "optics_preview(df=df2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def agglomerative_clustering_preview(df: pd.DataFrame, clusters: List[int] = [3, 4, 5]):\r\n",
    "    for c in clusters:\r\n",
    "        model = AgglomerativeClustering(n_clusters=c)\r\n",
    "        data = df[['Annual Income', 'Age', 'Spending Score']]\r\n",
    "        y_hat = model.fit_predict(X=data).astype(str)\r\n",
    "        label = f'Agg Clustering: {c} clusters: {silhouette_score(X=data, labels=y_hat):.2f}'\r\n",
    "        fig = px.scatter_3d(data_frame=df, x='Annual Income', y='Age', z='Spending Score', color=y_hat, title=label)\r\n",
    "        fig.show()\r\n",
    "\r\n",
    "agglomerative_clustering_preview(df=df2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def affinity_propagation_preview(df: pd.DataFrame, damping_factor_space = np.linspace(0.5, 0.9, 3)):\r\n",
    "    for d in damping_factor_space:\r\n",
    "        model = AffinityPropagation(damping=d)\r\n",
    "        data = df[['Annual Income', 'Age', 'Spending Score']]\r\n",
    "        y_hat = model.fit_predict(X=data).astype(str)\r\n",
    "        label = f'Aff Propagation: damping={d}: {silhouette_score(X=data, labels=y_hat):.2f}'\r\n",
    "        fig = px.scatter_3d(data_frame=df, x='Annual Income', y='Age', z='Spending Score', color=y_hat, title=label)\r\n",
    "        fig.show()\r\n",
    "\r\n",
    "affinity_propagation_preview(df=df2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Numerical Optimisation - Hyperparameter Selection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_dist_score(df: pd.DataFrame, clusters: int, plot: bool = False):\r\n",
    "    model = KMeans(n_clusters=clusters, random_state=3).fit(X=df)\r\n",
    "    centers = model.cluster_centers_\r\n",
    "    y_hat = model.predict(X=df)\r\n",
    "    df_tmp = df.copy()\r\n",
    "    df_tmp['Class'] = y_hat\r\n",
    "    means = []\r\n",
    "    counts = []\r\n",
    "    for i, c in enumerate(centers):\r\n",
    "        class_i = df_tmp[df_tmp['Class'] == i].drop(columns='Class')\r\n",
    "        distances = np.linalg.norm(class_i - centers[i, :], axis=1)\r\n",
    "        means.append(distances.mean())\r\n",
    "        counts.append(len(class_i))\r\n",
    "    \r\n",
    "    if plot == True:\r\n",
    "        for i, m in enumerate(means):\r\n",
    "            plt.plot(centers[i,0], centers[i,1], 'o', mfc='none', color='r', markersize=m * 5)\r\n",
    "        sns.scatterplot(x=df['Annual Income'], y=df['Spending Score'], hue=y_hat)\r\n",
    "\r\n",
    "    return np.std(means) / (sum(means) / len(means)), np.std(counts)\r\n",
    "    # return np.std(means)\r\n",
    "\r\n",
    "get_dist_score(df2_s, 6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_silhouette_score(df: pd.DataFrame, clusters: int):\r\n",
    "    from sklearn.metrics import silhouette_score\r\n",
    "    X = df\r\n",
    "    y_hat = KMeans(n_clusters=clusters).fit(X=X).predict(X=X)\r\n",
    "    return silhouette_score(X=df, labels=y_hat)\r\n",
    "\r\n",
    "get_silhouette_score(df=df2_s, clusters=6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# type: ignore\r\n",
    "from sklearn.metrics import silhouette_score\r\n",
    "\r\n",
    "def get_custom_score_plot(df: pd.DataFrame, clusters: List[int] = list(range(2, 11))):\r\n",
    "    distance_scores = [get_dist_score(df, n)[0] for n in clusters]\r\n",
    "    count_scores = [get_dist_score(df, n)[1] for n in clusters]\r\n",
    "    silhouette_scores = [get_silhouette_score(df, n) for n in clusters]\r\n",
    "    \r\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(12, 4))\r\n",
    "    with sns.axes_style(style='darkgrid'):\r\n",
    "        ax[0].plot(clusters, silhouette_scores, color='b')\r\n",
    "        ax[0].set_title('Silhouette Scores')\r\n",
    "        ax[0].set_xlabel(xlabel='No. of Clusters')\r\n",
    "\r\n",
    "        ax[1].plot(clusters, distance_scores, color='g')\r\n",
    "        ax[1].set_title('Distance Scores')\r\n",
    "        ax[1].set_xlabel(xlabel='No. of Clusters')\r\n",
    "\r\n",
    "        ax[2].plot(clusters, count_scores, color='r')\r\n",
    "        ax[2].set_title('Count Scores')\r\n",
    "        ax[2].set_xlabel(xlabel='No. of Clusters')\r\n",
    "\r\n",
    "get_custom_score_plot(df=df2_s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import plotly.express as px\r\n",
    "\r\n",
    "def get_final_plot(df: pd.DataFrame, save_model: bool = False):\r\n",
    "    df_tmp = df.copy()\r\n",
    "    model = KMeans(n_clusters=6, random_state=7)\r\n",
    "    model.fit(X=df_tmp)\r\n",
    "    if save_model:\r\n",
    "        pickle.dump(obj=model, file=open('./out/customer-clustering-model.p', 'wb'))\r\n",
    "    y_hat = model.predict(X=df_tmp).astype(str)\r\n",
    "    df_tmp['Class'] = y_hat\r\n",
    "    fig = px.scatter_3d(data_frame=df_tmp, x='Age', y='Annual Income', z='Spending Score', color='Class')\r\n",
    "    fig.update_traces(marker=dict(size=6))\r\n",
    "    fig.show()\r\n",
    "\r\n",
    "get_final_plot(df2_s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_final_plot(df2)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}